"""
EJECUTAR MODELO TOP2VEC
=======================

Script principal para ejecutar el an√°lisis de t√≥picos en noticias econ√≥micas.
Dise√±ado para ser simple de usar - solo ejecuta este archivo.

Instrucciones:
1. Abre una terminal (PowerShell o CMD)
2. Navega a esta carpeta: cd top2vec_para_economistas
3. Ejecuta: uv run python ejecutar_modelo.py

Los resultados se guardar√°n en la carpeta 'resultados/'
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime

# A√±adir el directorio padre al path para importar top2vec
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from top2vec import Top2Vec
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import strip_tags

# Importar configuraci√≥n
from configuracion import *

# =============================================================================
# FUNCIONES AUXILIARES
# =============================================================================

def spanish_friendly_tokenizer(document):
    """
    Tokenizer que preserva tildes y √± para espa√±ol
    """
    clean_text = strip_tags(document)
    return simple_preprocess(clean_text, deacc=False)


class PrecomputedEmbeddings:
    """
    Clase para proveer embeddings precomputados a Top2Vec
    (No necesitas entender esto - es c√≥digo interno)
    """
    
    def __init__(self, embeddings_file, csv_file=None):
        print(f"üìÇ Cargando embeddings desde: {embeddings_file}")
        data = np.load(embeddings_file, allow_pickle=True)
        self.embeddings = data['embeddings']
        self.pub_dates = data['pub_date']
        self.doc_ids = data['doc_id']
        
        print(f"‚úÖ Embeddings cargados:")
        print(f"   ‚Ä¢ Documentos: {len(self.embeddings):,}")
        print(f"   ‚Ä¢ Dimensiones: {self.embeddings.shape[1]}")
        
        self.documents = None
        if csv_file and os.path.exists(csv_file):
            print(f"üìÑ Cargando textos desde: {csv_file}")
            print(f"   ‚è≥ Este paso puede tomar varios minutos (archivo grande)...")
            df = pd.read_csv(csv_file)
            
            # Verificar que la columna existe
            if COLUMNA_TEXTO not in df.columns:
                raise ValueError(f"‚ùå La columna '{COLUMNA_TEXTO}' no existe en el CSV. "
                               f"Columnas disponibles: {list(df.columns)}")
            
            self.documents = df[COLUMNA_TEXTO].astype(str).tolist()
            print(f"   ‚úÖ Textos cargados: {len(self.documents):,}")
        
        self.current_batch_start = 0
    
    def __call__(self, documents_batch):
        """M√©todo para que Top2Vec pueda llamar a esta clase"""
        batch_size = len(documents_batch)
        start_idx = self.current_batch_start
        end_idx = min(start_idx + batch_size, len(self.embeddings))
        
        batch_embeddings = self.embeddings[start_idx:end_idx]
        self.current_batch_start = end_idx
        
        return batch_embeddings


def imprimir_banner():
    """Imprime un banner bonito al inicio"""
    print("\n" + "="*70)
    print("  üìä TOP2VEC - AN√ÅLISIS DE T√ìPICOS EN NOTICIAS ECON√ìMICAS")
    print("="*70)
    print(f"  üïê Inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70 + "\n")


def imprimir_configuracion():
    """Muestra la configuraci√≥n que se est√° usando"""
    print("\nüìã CONFIGURACI√ìN ACTUAL:")
    print("-" * 50)
    print(f"  HDBSCAN:")
    for key, value in HDBSCAN_CONFIG.items():
        print(f"    ‚Ä¢ {key}: {value}")
    print(f"\n  UMAP:")
    for key, value in UMAP_CONFIG.items():
        print(f"    ‚Ä¢ {key}: {value}")
    print(f"\n  Otros:")
    print(f"    ‚Ä¢ topic_merge_delta: {TOPIC_MERGE_DELTA}")
    print(f"    ‚Ä¢ min_count: {MIN_COUNT_PALABRAS}")
    print("-" * 50 + "\n")


def crear_modelo_top2vec():
    """
    Funci√≥n principal que crea y entrena el modelo Top2Vec
    """
    
    # Validar que los archivos existen
    if not os.path.exists(ARCHIVO_EMBEDDINGS):
        raise FileNotFoundError(f"‚ùå No se encuentra el archivo: {ARCHIVO_EMBEDDINGS}")
    
    if not os.path.exists(ARCHIVO_NOTICIAS):
        raise FileNotFoundError(f"‚ùå No se encuentra el archivo: {ARCHIVO_NOTICIAS}")
    
    # Cargar embeddings y textos
    print("\nüîÑ PASO 1: Cargando datos...")
    print("-" * 50)
    embedding_provider = PrecomputedEmbeddings(ARCHIVO_EMBEDDINGS, ARCHIVO_NOTICIAS)
    
    # Preparar documentos
    if embedding_provider.documents:
        documents = embedding_provider.documents
        print(f"‚úÖ Usando {len(documents):,} documentos con texto completo")
    else:
        documents = [f"Document {i}" for i in range(len(embedding_provider.embeddings))]
        print(f"‚ö†Ô∏è  Usando {len(documents):,} documentos placeholder (sin texto)")
    
    # Preparar IDs
    document_ids = [str(doc_id) for doc_id in embedding_provider.doc_ids.tolist()]
    print(f"‚úÖ {len(document_ids):,} IDs de documentos preparados")
    
    # Entrenar modelo
    print("\nüöÄ PASO 2: Entrenando modelo Top2Vec...")
    print("-" * 50)
    print("‚è≥ Este proceso puede tomar 15-30 minutos dependiendo del tama√±o...")
    print("   (Los embeddings ya est√°n calculados, solo falta el clustering)")
    print()
    
    try:
        model = Top2Vec(
            documents=documents,
            embedding_model=embedding_provider,
            document_ids=document_ids,
            tokenizer=spanish_friendly_tokenizer if USE_SPANISH_TOKENIZER else None,
            min_count=MIN_COUNT_PALABRAS,
            umap_args=UMAP_CONFIG,
            hdbscan_args=HDBSCAN_CONFIG,
            topic_merge_delta=TOPIC_MERGE_DELTA,
            use_embedding_model_tokenizer=USE_EMBEDDING_MODEL_TOKENIZER,
            verbose=VERBOSE
        )
        
        print(f"\n‚úÖ MODELO ENTRENADO EXITOSAMENTE!")
        print("=" * 50)
        print(f"  üìä T√≥picos encontrados: {model.get_num_topics()}")
        print(f"  üìÑ Documentos procesados: {len(documents):,}")
        print("=" * 50)
        
        return model, embedding_provider
        
    except Exception as e:
        print(f"\n‚ùå ERROR durante el entrenamiento:")
        print(f"   {str(e)}")
        print("\nüí° Sugerencias:")
        print("   ‚Ä¢ Verifica que tienes suficiente memoria RAM")
        print("   ‚Ä¢ Intenta aumentar min_cluster_size en configuracion.py")
        print("   ‚Ä¢ Revisa que los archivos de datos est√©n completos")
        raise


def exportar_resultados(model):
    """
    Exporta los resultados del modelo a archivos f√°ciles de leer
    """
    print("\nüíæ PASO 3: Exportando resultados...")
    print("-" * 50)
    
    # Crear carpeta de resultados si no existe
    os.makedirs(CARPETA_RESULTADOS, exist_ok=True)
    
    # Obtener informaci√≥n de los t√≥picos
    num_topics = model.get_num_topics()
    topic_sizes, topic_nums = model.get_topic_sizes()
    
    # Crear DataFrame con resumen de t√≥picos
    resultados = []
    
    for topic_num in topic_nums:
        # Obtener palabras y scores del t√≥pico
        words, word_scores, _ = model.get_topics(topic_nums=[topic_num])
        
        # Tomar solo las N palabras m√°s relevantes
        top_words = words[0][:NUM_PALABRAS_POR_TOPICO]
        top_scores = word_scores[0][:NUM_PALABRAS_POR_TOPICO]
        
        # Crear fila para este t√≥pico
        fila = {
            'topic_id': topic_num,
            'num_documentos': topic_sizes[topic_nums.tolist().index(topic_num)],
            'palabras_clave': ', '.join(top_words),
        }
        
        # Agregar cada palabra y su score como columnas separadas
        for i, (word, score) in enumerate(zip(top_words, top_scores), 1):
            fila[f'palabra_{i}'] = word
            fila[f'score_palabra_{i}'] = round(score, 4)
        
        resultados.append(fila)
    
    # Convertir a DataFrame
    df_resultados = pd.DataFrame(resultados)
    
    # Ordenar por n√∫mero de documentos (t√≥picos m√°s grandes primero)
    df_resultados = df_resultados.sort_values('num_documentos', ascending=False)
    
    # Exportar a Excel
    if EXPORTAR_EXCEL:
        archivo_excel = os.path.join(CARPETA_RESULTADOS, 'resumen_topicos.xlsx')
        df_resultados.to_excel(archivo_excel, index=False)
        print(f"‚úÖ Resumen exportado a: {archivo_excel}")
    
    # Tambi√©n exportar a CSV (m√°s liviano)
    archivo_csv = os.path.join(CARPETA_RESULTADOS, 'resumen_topicos.csv')
    df_resultados.to_csv(archivo_csv, index=False)
    print(f"‚úÖ Resumen exportado a: {archivo_csv}")
    
    # Crear un archivo de texto con resumen legible
    archivo_txt = os.path.join(CARPETA_RESULTADOS, 'resumen_topicos.txt')
    with open(archivo_txt, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("  RESUMEN DE T√ìPICOS - TOP2VEC\n")
        f.write(f"  Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 70 + "\n\n")
        f.write(f"Total de t√≥picos encontrados: {num_topics}\n")
        f.write(f"Total de documentos clasificados: {sum(topic_sizes):,}\n\n")
        f.write("=" * 70 + "\n\n")
        
        for _, row in df_resultados.iterrows():
            f.write(f"T√ìPICO {row['topic_id']} ({row['num_documentos']:,} documentos)\n")
            f.write("-" * 70 + "\n")
            f.write("Palabras clave (con relevancia):\n")
            for i in range(1, NUM_PALABRAS_POR_TOPICO + 1):
                if f'palabra_{i}' in row:
                    palabra = row[f'palabra_{i}']
                    score = row[f'score_palabra_{i}']
                    f.write(f"  {i}. {palabra:<20} (relevancia: {score:.3f})\n")
            f.write("\n" + "=" * 70 + "\n\n")
    
    print(f"‚úÖ Resumen legible exportado a: {archivo_txt}")
    
    # Crear un resumen estad√≠stico
    print(f"\nüìä ESTAD√çSTICAS:")
    print(f"   ‚Ä¢ T√≥picos totales: {num_topics}")
    print(f"   ‚Ä¢ Documentos clasificados: {sum(topic_sizes):,}")
    print(f"   ‚Ä¢ Promedio docs/t√≥pico: {sum(topic_sizes) / num_topics:.1f}")
    print(f"   ‚Ä¢ T√≥pico m√°s grande: {max(topic_sizes):,} documentos")
    print(f"   ‚Ä¢ T√≥pico m√°s peque√±o: {min(topic_sizes):,} documentos")
    
    return df_resultados


def guardar_modelo(model):
    """Guarda el modelo entrenado para uso futuro"""
    if GUARDAR_MODELO:
        print(f"\nüíæ PASO 4: Guardando modelo...")
        print("-" * 50)
        
        os.makedirs(CARPETA_MODELOS, exist_ok=True)
        ruta_modelo = os.path.join(CARPETA_MODELOS, NOMBRE_MODELO)
        
        model.save(ruta_modelo)
        print(f"‚úÖ Modelo guardado en: {ruta_modelo}")
        print(f"   Podr√°s reutilizar este modelo sin re-entrenar")


def imprimir_resumen_final():
    """Imprime un resumen final al completar"""
    print("\n" + "=" * 70)
    print("  ‚úÖ PROCESO COMPLETADO EXITOSAMENTE")
    print("=" * 70)
    print(f"  üïê Finalizado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    print(f"\nüìÇ Revisa los resultados en la carpeta: {CARPETA_RESULTADOS}/")
    print("   ‚Ä¢ resumen_topicos.xlsx - Tabla completa en Excel")
    print("   ‚Ä¢ resumen_topicos.csv - Tabla en formato CSV")
    print("   ‚Ä¢ resumen_topicos.txt - Resumen legible en texto")
    
    if GUARDAR_MODELO:
        print(f"\nü§ñ Modelo guardado en: {CARPETA_MODELOS}/{NOMBRE_MODELO}")
    
    print("\nüí° Pr√≥ximos pasos:")
    print("   1. Abre el archivo Excel para ver los t√≥picos")
    print("   2. Si quieres ajustar par√°metros, edita configuracion.py")
    print("   3. Vuelve a ejecutar este script para probar nuevas configuraciones")
    print("\n")


# =============================================================================
# FUNCI√ìN PRINCIPAL
# =============================================================================

def main():
    """
    Funci√≥n principal que ejecuta todo el proceso
    """
    try:
        # Banner inicial
        imprimir_banner()
        
        # Mostrar configuraci√≥n
        imprimir_configuracion()
        
        # Crear y entrenar modelo
        model, embedding_provider = crear_modelo_top2vec()
        
        # Exportar resultados
        df_resultados = exportar_resultados(model)
        
        # Guardar modelo
        guardar_modelo(model)
        
        # Resumen final
        imprimir_resumen_final()
        
        return model, df_resultados
        
    except Exception as e:
        print("\n" + "=" * 70)
        print("  ‚ùå ERROR EN LA EJECUCI√ìN")
        print("=" * 70)
        print(f"\n{str(e)}\n")
        print("üí° Revisa la secci√≥n 'Soluci√≥n de Problemas' en README.md")
        print("=" * 70 + "\n")
        raise


# =============================================================================
# PUNTO DE ENTRADA
# =============================================================================

if __name__ == "__main__":
    model, resultados = main()