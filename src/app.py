"""
Top2Vec Web App - Aplicaci√≥n para Economistas
==============================================

Aplicaci√≥n web profesional para entrenar y explorar modelos Top2Vec
sin necesidad de programar.

Autor: Top2Vec Team
Fecha: Noviembre 2025
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import os
import json
import sys
from datetime import datetime
from pathlib import Path
import time
import psutil
from io import BytesIO
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from PIL import Image

# A√±adir el directorio padre al path para importar top2vec
sys.path.append(str(Path(__file__).parent.parent))

from top2vec import Top2Vec
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import strip_tags

# =============================================================================
# CONFIGURACI√ìN DE LA P√ÅGINA
# =============================================================================

st.set_page_config(
    page_title="Top2Vec - An√°lisis de T√≥picos",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# =============================================================================
# ESTILOS CSS PERSONALIZADOS
# =============================================================================

st.markdown("""
<style>
    /* Reducir m√°rgenes laterales para aprovechar m√°s espacio */
    .block-container {
        padding-left: 2rem;
        padding-right: 2rem;
        max-width: 95%;
    }
    
    /* Tema principal */
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        font-weight: bold;
        margin-bottom: 1rem;
    }
    
    /* M√©tricas destacadas */
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        margin: 0.5rem 0;
    }
    
    .metric-value {
        font-size: 2rem;
        font-weight: bold;
        margin: 0.5rem 0;
    }
    
    .metric-label {
        font-size: 0.9rem;
        opacity: 0.9;
    }
    
    /* Tarjetas de informaci√≥n */
    .info-card {
        background-color: #f0f2f6;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 5px solid #1f77b4;
        margin: 1rem 0;
    }
    
    /* Botones personalizados */
    .stButton > button {
        width: 100%;
        background-color: #1f77b4;
        color: white;
        border-radius: 5px;
        padding: 0.5rem 1rem;
        font-weight: bold;
        border: none;
        transition: all 0.3s;
    }
    
    .stButton > button:hover {
        background-color: #145a8c;
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    
    /* Tabs personalizados */
    .stTabs [data-baseweb="tab-list"] {
        gap: 24px;
    }
    
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding: 0 24px;
        background-color: #f0f2f6;
        border-radius: 5px 5px 0 0;
    }
    
    .stTabs [aria-selected="true"] {
        background-color: #1f77b4;
        color: white;
    }
    
    /* Barra de progreso */
    .stProgress > div > div > div > div {
        background-color: #1f77b4;
    }
    
    /* Sidebar */
    .css-1d391kg {
        background-color: #f8f9fa;
    }
    
    /* Tooltips */
    .tooltip {
        position: relative;
        display: inline-block;
        cursor: help;
    }
    
    /* Alertas */
    .alert-success {
        background-color: #d4edda;
        border-left: 5px solid #28a745;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    
    .alert-warning {
        background-color: #fff3cd;
        border-left: 5px solid #ffc107;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    
    .alert-error {
        background-color: #f8d7da;
        border-left: 5px solid #dc3545;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# =============================================================================
# UTILIDADES Y FUNCIONES AUXILIARES
# =============================================================================

def spanish_friendly_tokenizer(document):
    """Tokenizer que preserva tildes y √± para espa√±ol"""
    clean_text = strip_tags(document)
    return simple_preprocess(clean_text, deacc=False)


class PrecomputedEmbeddings:
    """Clase para proveer embeddings precomputados a Top2Vec"""
    
    def __init__(self, embeddings_file, csv_file=None):
        """
        Cargar embeddings precomputados
        
        Args:
            embeddings_file: Archivo .npz con embeddings
            csv_file: CSV original para obtener textos y fechas
        """
        self.embeddings_file = embeddings_file
        
        # Cargar embeddings
        data = np.load(embeddings_file, allow_pickle=True)
        
        # CR√çTICO: El archivo NPZ usa 'embeddings' NO 'document_vectors'
        # pero el nuevo archivo usa 'document_vectors', soportar ambos
        if 'embeddings' in data:
            self.embeddings = data['embeddings']
        elif 'document_vectors' in data:
            self.embeddings = data['document_vectors']
        else:
            raise ValueError(f"No se encontraron embeddings. Claves: {list(data.keys())}")
        
        # Cargar word_vectors y vocab (CR√çTICO para Top2Vec)
        self.word_vectors = data.get('word_vectors', None)
        self.vocab = data.get('vocab', None)
        self.word_indexes = data.get('word_indexes', None)
        if self.word_indexes is not None:
            self.word_indexes = self.word_indexes.item()  # Convertir de numpy a dict
        
        # Cargar pub_dates y doc_ids del CSV
        self.documents = None
        self.pub_dates = None
        self.doc_ids = None
        
        if csv_file and os.path.exists(csv_file):
            df = pd.read_csv(csv_file)
            self.documents = df['body'].astype(str).tolist()
            
            # Cargar fechas y doc_ids
            if 'pub_date' in df.columns:
                self.pub_dates = pd.to_datetime(df['pub_date']).values
            if 'doc_id' in df.columns:
                self.doc_ids = df['doc_id'].values
            else:
                self.doc_ids = np.arange(len(self.documents))
        
        # √çndice para mapear textos a embeddings
        self.current_batch_start = 0
    
    def __call__(self, documents_batch):
        """
        M√©todo para que Top2Vec pueda llamar a esta clase como embedding_model
        
        Args:
            documents_batch: Lista de documentos para embeddings
            
        Returns:
            numpy.ndarray: Embeddings correspondientes
        """
        batch_size = len(documents_batch)
        
        # Retornar el siguiente lote de embeddings
        start_idx = self.current_batch_start
        end_idx = start_idx + batch_size
        
        if end_idx > len(self.embeddings):
            end_idx = len(self.embeddings)
            
        batch_embeddings = self.embeddings[start_idx:end_idx]
        self.current_batch_start = end_idx
        
        return batch_embeddings


def get_system_resources():
    """Obtiene informaci√≥n de uso de recursos del sistema"""
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    
    return {
        'cpu_percent': cpu_percent,
        'memory_percent': memory.percent,
        'memory_used_gb': memory.used / (1024**3),
        'memory_total_gb': memory.total / (1024**3)
    }


def save_model_metadata(config, model_path, num_topics, execution_time):
    """Guarda metadata del modelo entrenado"""
    metadata = {
        'timestamp': datetime.now().isoformat(),
        'model_path': str(model_path),
        'num_topics': num_topics,
        'execution_time_seconds': execution_time,
        'config': config
    }
    
    metadata_path = Path(model_path).parent / 'metadata.json'
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    return metadata_path


def load_model_metadata(model_dir):
    """Carga metadata de un modelo"""
    metadata_path = Path(model_dir) / 'metadata.json'
    if metadata_path.exists():
        with open(metadata_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None


def list_available_models():
    """Lista todos los modelos disponibles en la carpeta de modelos"""
    models_dir = Path('modelos')
    if not models_dir.exists():
        return []
    
    models = []
    for item in models_dir.iterdir():
        if item.is_dir():
            metadata = load_model_metadata(item)
            if metadata:
                models.append({
                    'name': item.name,
                    'path': str(item),
                    'metadata': metadata
                })
    
    return sorted(models, key=lambda x: x['metadata']['timestamp'], reverse=True)


def clean_topic_words(words, scores, target_count=20):
    """
    Limpia y deduplicar palabras de t√≥picos eliminando variantes con puntuaci√≥n.
    
    Args:
        words: Lista de palabras del t√≥pico
        scores: Lista de scores correspondientes
        target_count: N√∫mero de palabras √∫nicas a retornar
    
    Returns:
        Tupla (palabras_limpias, scores_limpios)
    """
    import re
    stop_suffixes = ('.el', '.la', '.de', '.y', '.en', '.los', '.las', '.un', '.una', '.al', '.del', '.por', '.con', '.sin', '.para', '.sobre', '.entre', '.o', '.u')
    seen_clean = {}
    for word, score in zip(words, scores):
        # Quitar puntuaci√≥n perif√©rica
        cleaned = re.sub(r'^[^\w]+|[^\w]+$', '', word, flags=re.UNICODE)
        # Quitar palabras con punto seguido de letras (ej: ".el", ".de") o signos
        if re.search(r'\.[a-z√°√©√≠√≥√∫√±]+$', cleaned, re.IGNORECASE) or cleaned.lower().endswith(stop_suffixes):
            cleaned = cleaned.split('.')[0]
        # Quitar palabras que contienen signos de puntuaci√≥n internos o finales
        if re.search(r'[\?\!\-\(\)\[\]"\'\;\:\,]', cleaned):
            cleaned = re.sub(r'[\?\!\-\(\)\[\]"\'\;\:\,]', '', cleaned)
        # Filtrar palabras que contienen n√∫meros
        if re.search(r'\d', cleaned):
            continue
        # Si la versi√≥n limpia est√° vac√≠a, skip
        if not cleaned:
            continue
        if cleaned.lower() not in seen_clean or score > seen_clean[cleaned.lower()][1]:
            seen_clean[cleaned.lower()] = (cleaned, score)
        if len(seen_clean) >= target_count:
            break
    items = sorted(seen_clean.values(), key=lambda x: x[1], reverse=True)
    clean_words = [item[0] for item in items]
    clean_scores = [item[1] for item in items]
    return clean_words, clean_scores


def create_wordcloud_image(words, scores, width=800, height=400, max_words=10):
    """Crea una imagen de wordcloud"""
    # Limitar a top N palabras
    words = words[:max_words]
    scores = scores[:max_words]
    
    # Crear diccionario de frecuencias
    word_freq = {word: score for word, score in zip(words, scores)}
    
    # Generar wordcloud con semilla fija para reproducibilidad
    wc = WordCloud(
        width=width,
        height=height,
        background_color='white',
        colormap='viridis',
        relative_scaling=0.5,
        min_font_size=10,
        max_words=max_words,
        random_state=42  # Semilla fija para que siempre genere el mismo layout
    ).generate_from_frequencies(word_freq)
    
    # Convertir a imagen
    fig, ax = plt.subplots(figsize=(width/100, height/100), dpi=100)
    ax.imshow(wc, interpolation='bilinear')
    ax.axis('off')
    plt.tight_layout(pad=0)
    
    # Guardar en buffer
    buf = BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', dpi=100)
    buf.seek(0)
    plt.close()
    
    return buf


def export_to_excel(model, pub_dates):
    """Exporta todos los resultados a un archivo Excel"""
    output = BytesIO()
    
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        # Hoja 1: Resumen de t√≥picos
        topic_sizes, topic_nums = model.get_topic_sizes()
        
        # Obtener todas las palabras de los t√≥picos una sola vez
        all_topic_words, all_word_scores, _ = model.get_topics()
        
        resumen_data = []
        for i, topic_num in enumerate(topic_nums):
            words = all_topic_words[i]
            word_scores = all_word_scores[i]
            
            # Limpiar y deduplicar palabras
            clean_words, clean_scores = clean_topic_words(words, word_scores, target_count=10)
            
            resumen_data.append({
                'topic_id': topic_num,
                'num_documentos': topic_sizes[i],
                'palabras_clave': ', '.join(clean_words),
                **{f'palabra_{j+1}': clean_words[j] if j < len(clean_words) else '' for j in range(10)},
                **{f'score_{j+1}': round(clean_scores[j], 4) if j < len(clean_scores) else 0 for j in range(10)}
            })
        
        df_resumen = pd.DataFrame(resumen_data)
        df_resumen.to_excel(writer, sheet_name='Resumen_Topicos', index=False)
        
        # Hoja 2: Series temporales
        # Usar directamente doc_top que ya fue calculado por Top2Vec
        topic_assignments = model.doc_top
        
        df_temporal = pd.DataFrame({
            'fecha': pd.to_datetime(pub_dates),
            'topico': topic_assignments
        })
        
        pivot = df_temporal.groupby([df_temporal['fecha'].dt.date, 'topico']).size().unstack(fill_value=0)
        pivot.to_excel(writer, sheet_name='Series_Temporales')
        
        # Hoja 3: Metadata
        metadata = pd.DataFrame([{
            'total_topicos': len(topic_nums),
            'total_documentos': len(all_doc_ids),
            'fecha_generacion': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }])
        metadata.to_excel(writer, sheet_name='Metadata', index=False)
    
    output.seek(0)
    return output


# =============================================================================
# INTERFAZ PRINCIPAL
# =============================================================================

def main():
    # Header
    st.markdown('<h1 class="main-header">üìä Top2Vec - An√°lisis de T√≥picos para Economistas</h1>', 
                unsafe_allow_html=True)
    
    # Inicializar session state
    if 'trained_model' not in st.session_state:
        st.session_state.trained_model = None
    if 'trained_model_data' not in st.session_state:
        st.session_state.trained_model_data = None
    if 'current_model' not in st.session_state:
        st.session_state.current_model = None
    if 'current_model_data' not in st.session_state:
        st.session_state.current_model_data = None
    
    # Tabs principales
    tab1, tab2, tab3 = st.tabs(["üéØ Entrenar Modelo", "üìä Explorar Resultados", "üìñ Ayuda y Documentaci√≥n"])
    
    with tab1:
        render_training_tab()
    
    with tab2:
        render_exploration_tab()
    
    with tab3:
        render_help_tab()


# =============================================================================
# TAB 1: ENTRENAR MODELO
# =============================================================================

def render_training_tab():
    st.markdown("### üéØ Configuraci√≥n y Entrenamiento del Modelo")
    
    st.markdown("""
    <div class="info-card">
        <strong>‚ÑπÔ∏è Informaci√≥n:</strong><br>
        Los embeddings ya est√°n precalculados, por lo que el entrenamiento tomar√° aproximadamente 15-30 minutos.
        Durante el proceso ver√°s actualizaciones en tiempo real del progreso.
    </div>
    """, unsafe_allow_html=True)
    
    # Configuraci√≥n en columnas
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown("#### üìã Configuraci√≥n B√°sica")
        
        # Filtro de datos
        st.markdown("##### üìÖ Filtro de Datos (opcional)")
        
        use_date_filter = st.checkbox(
            "Filtrar por rango de fechas",
            value=False,
            help="Entrenar solo con un subset de datos para pruebas r√°pidas"
        )
        
        start_year = None
        end_year = None
        
        if use_date_filter:
            col_start, col_end = st.columns(2)
            with col_start:
                start_year = st.number_input(
                    "A√±o inicial",
                    min_value=2008,
                    max_value=2024,
                    value=2020,
                    step=1,
                    help="Ejemplo: 2020 para entrenar desde 2020 en adelante"
                )
            with col_end:
                end_year = st.number_input(
                    "A√±o final",
                    min_value=2008,
                    max_value=2024,
                    value=2024,
                    step=1,
                    help="Ejemplo: 2024 para entrenar hasta 2024"
                )
            
            st.info(f"üìä Se entrenar√°n solo documentos entre {start_year} y {end_year}")
        
        # Presets
        preset = st.selectbox(
            "Preset de Configuraci√≥n",
            ["An√°lisis General (Recomendado)", "Temas Emergentes", "Macro-Temas", "Personalizado"],
            help="Selecciona un preset predefinido o personaliza los par√°metros"
        )
        
        # Nombre del modelo
        model_name = st.text_input(
            "Nombre del Modelo",
            value=f"modelo_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            help="Nombre √∫nico para identificar este modelo"
        )
        
        # Archivos de datos
        st.markdown("##### üìÇ Archivos de Datos")
        data_file = st.text_input("Archivo de Noticias (CSV)", value="data/noticias.csv")
        embeddings_file = st.text_input("Archivo de Embeddings (NPZ)", value="data/embeddings_precalculados.npz")
    
    with col2:
        st.markdown("#### ‚öôÔ∏è Par√°metros del Modelo")
        
        show_advanced = st.checkbox("Mostrar par√°metros avanzados", value=False)
        
        # Cargar preset
        if preset == "An√°lisis General (Recomendado)":
            default_config = {
                'min_cluster_size': 50,
                'min_samples': 25,
                'n_neighbors': 50,
                'n_components': 5,
                'topic_merge_delta': 0.1
            }
        elif preset == "Temas Emergentes":
            default_config = {
                'min_cluster_size': 30,
                'min_samples': 15,
                'n_neighbors': 30,
                'n_components': 5,
                'topic_merge_delta': 0.08
            }
        elif preset == "Macro-Temas":
            default_config = {
                'min_cluster_size': 75,
                'min_samples': 40,
                'n_neighbors': 70,
                'n_components': 5,
                'topic_merge_delta': 0.12
            }
        else:
            default_config = {
                'min_cluster_size': 50,
                'min_samples': 25,
                'n_neighbors': 50,
                'n_components': 5,
                'topic_merge_delta': 0.1
            }
        
        if show_advanced:
            st.markdown("##### üîç HDBSCAN (Agrupaci√≥n)")
            
            min_cluster_size = st.slider(
                "Tama√±o M√≠nimo de Cluster",
                min_value=10, max_value=200, value=default_config['min_cluster_size'], step=5,
                help="M√≠nimo de documentos para formar un t√≥pico. ‚Üë = t√≥picos m√°s grandes"
            )
            
            min_samples = st.slider(
                "Muestras M√≠nimas",
                min_value=5, max_value=100, value=default_config['min_samples'], step=5,
                help="Densidad m√≠nima para identificar un t√≥pico. ‚Üë = t√≥picos m√°s robustos"
            )
            
            st.markdown("##### üó∫Ô∏è UMAP (Reducci√≥n Dimensional)")
            
            n_neighbors = st.slider(
                "N√∫mero de Vecinos",
                min_value=10, max_value=200, value=default_config['n_neighbors'], step=10,
                help="Vecinos cercanos a considerar. ‚Üë = estructura global, ‚Üì = estructura local"
            )
            
            n_components = st.slider(
                "Componentes",
                min_value=2, max_value=10, value=default_config['n_components'], step=1,
                help="Dimensiones en el espacio reducido (t√≠picamente 2-10)"
            )
            
            st.markdown("##### üîó Fusi√≥n de T√≥picos")
            
            topic_merge_delta = st.slider(
                "Delta de Fusi√≥n",
                min_value=0.01, max_value=0.30, value=default_config['topic_merge_delta'], step=0.01,
                help="Similitud m√≠nima para fusionar t√≥picos. ‚Üë = fusiona m√°s"
            )
        else:
            min_cluster_size = default_config['min_cluster_size']
            min_samples = default_config['min_samples']
            n_neighbors = default_config['n_neighbors']
            n_components = default_config['n_components']
            topic_merge_delta = default_config['topic_merge_delta']
            
            st.info(f"""
            **Usando preset '{preset}':**
            - Min Cluster Size: {min_cluster_size}
            - Min Samples: {min_samples}
            - N Neighbors: {n_neighbors}
            - N Components: {n_components}
            - Topic Merge Delta: {topic_merge_delta}
            """)
    
    # Bot√≥n de entrenamiento
    st.markdown("---")
    
    col_btn1, col_btn2, col_btn3 = st.columns([1, 1, 1])
    
    with col_btn2:
        train_button = st.button("üöÄ Entrenar Modelo", type="primary", use_container_width=True)
    
    if train_button:
        train_model(
            model_name=model_name,
            data_file=data_file,
            embeddings_file=embeddings_file,
            config={
                'min_cluster_size': min_cluster_size,
                'min_samples': min_samples,
                'n_neighbors': n_neighbors,
                'n_components': n_components,
                'topic_merge_delta': topic_merge_delta
            },
            date_filter={'start_year': start_year, 'end_year': end_year} if use_date_filter else None
        )


def train_model(model_name, data_file, embeddings_file, config, date_filter=None):
    """Ejecuta el entrenamiento del modelo con visualizaci√≥n de progreso"""
    
    # Contenedor de progreso
    progress_container = st.container()
    
    with progress_container:
        st.markdown("### üîÑ Entrenamiento en Progreso")
        
        # Barra de progreso
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # M√©tricas en tiempo real
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            metric_tiempo = st.empty()
        with col2:
            metric_cpu = st.empty()
        with col3:
            metric_memoria = st.empty()
        with col4:
            metric_eta = st.empty()
        
        # Log de mensajes
        log_container = st.expander("üìã Ver log detallado", expanded=True)
        log_text = log_container.empty()
        
        logs = []
        start_time = time.time()
        
        try:
            # Paso 1: Validar archivos
            progress_bar.progress(5)
            status_text.text("Validando archivos...")
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] Validando archivos...")
            log_text.text('\n'.join(logs[-20:]))
            
            if not os.path.exists(data_file):
                st.error(f"‚ùå No se encuentra el archivo: {data_file}")
                return
            
            if not os.path.exists(embeddings_file):
                st.error(f"‚ùå No se encuentra el archivo: {embeddings_file}")
                return
            
            # Paso 2: Cargar datos
            progress_bar.progress(10)
            status_text.text("Cargando embeddings y datos...")
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] Cargando embeddings...")
            log_text.text('\n'.join(logs[-20:]))
            
            embedding_provider = PrecomputedEmbeddings(embeddings_file, data_file)
            
            resources = get_system_resources()
            metric_cpu.metric("CPU", f"{resources['cpu_percent']:.1f}%")
            metric_memoria.metric("RAM", f"{resources['memory_percent']:.1f}%")
            
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ Embeddings cargados: {len(embedding_provider.embeddings):,} docs")
            log_text.text('\n'.join(logs[-20:]))
            
            # Paso 3: Preparar documentos
            progress_bar.progress(20)
            status_text.text("Preparando documentos...")
            
            # Aplicar filtro de fechas si est√° configurado
            if date_filter and date_filter['start_year'] and date_filter['end_year']:
                logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üìÖ Aplicando filtro de fechas: {date_filter['start_year']}-{date_filter['end_year']}")
                log_text.text('\n'.join(logs[-20:]))
                
                # Convertir pub_dates a datetime si no lo est√°
                pub_dates_dt = pd.to_datetime(embedding_provider.pub_dates)
                
                # Crear m√°scara de fechas
                start_date = pd.Timestamp(f"{date_filter['start_year']}-01-01")
                end_date = pd.Timestamp(f"{date_filter['end_year']}-12-31")
                
                date_mask = (pub_dates_dt >= start_date) & (pub_dates_dt <= end_date)
                indices_filtrados = np.where(date_mask)[0]
                
                # Filtrar todos los datos
                original_count = len(embedding_provider.embeddings)
                embedding_provider.embeddings = embedding_provider.embeddings[indices_filtrados]
                embedding_provider.pub_dates = embedding_provider.pub_dates[indices_filtrados]
                embedding_provider.doc_ids = embedding_provider.doc_ids[indices_filtrados]
                
                if embedding_provider.documents:
                    embedding_provider.documents = [embedding_provider.documents[i] for i in indices_filtrados]
                
                filtered_count = len(embedding_provider.embeddings)
                logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ Filtrado: {original_count:,} ‚Üí {filtered_count:,} docs ({filtered_count/original_count*100:.1f}%)")
                log_text.text('\n'.join(logs[-20:]))
            
            # IMPORTANTE: Usar documentos del NPZ (ya tokenizados/procesados)
            if embedding_provider.documents:
                documents = embedding_provider.documents
                logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ Textos cargados: {len(documents):,}")
            else:
                documents = [f"Document {i}" for i in range(len(embedding_provider.embeddings))]
                logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ‚ö†Ô∏è Usando placeholders")
            
            log_text.text('\n'.join(logs[-20:]))
            
            # CR√çTICO: Siempre convertir doc_ids a strings (igual que c√≥digo original)
            # Esto es necesario para que Top2Vec funcione correctamente con embeddings precomputados
            document_ids = [str(doc_id) for doc_id in embedding_provider.doc_ids.tolist()]
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üìã IDs de documentos: {len(document_ids)} (como strings)")
            log_text.text('\n'.join(logs[-20:]))
            
            # Paso 4: Entrenar modelo
            progress_bar.progress(30)
            status_text.text("Entrenando Top2Vec (esto puede tomar 15-30 minutos)...")
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üöÄ Iniciando entrenamiento Top2Vec...")
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] Configuraci√≥n:")
            logs.append(f"  ‚Ä¢ min_cluster_size: {config['min_cluster_size']}")
            logs.append(f"  ‚Ä¢ min_samples: {config['min_samples']}")
            logs.append(f"  ‚Ä¢ n_neighbors: {config['n_neighbors']}")
            logs.append(f"  ‚Ä¢ n_components: {config['n_components']}")
            logs.append(f"  ‚Ä¢ topic_merge_delta: {config['topic_merge_delta']}")
            log_text.text('\n'.join(logs[-20:]))
            
            # Actualizar m√©tricas cada 5 segundos durante el entrenamiento
            training_start = time.time()
            
            # Configuraci√≥n UMAP y HDBSCAN
            umap_args = {
                'n_neighbors': config['n_neighbors'],
                'n_components': config['n_components'],
                'metric': 'cosine',
                'random_state': 42
            }
            
            hdbscan_args = {
                'min_cluster_size': config['min_cluster_size'],
                'min_samples': config['min_samples'],
                'metric': 'euclidean',
                'cluster_selection_method': 'eom'
            }
            
            # Entrenar usando m√©todo del notebook (crear modelo vac√≠o y asignar atributos)
            # Este es el m√©todo que se us√≥ para crear el modelo funcional original
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üîß Creando modelo desde embeddings precomputados...")
            log_text.text('\n'.join(logs[-20:]))
            
            # Usar word_vectors y vocab del embedding_provider
            word_vectors = embedding_provider.word_vectors
            vocab = embedding_provider.vocab.tolist() if isinstance(embedding_provider.vocab, np.ndarray) else embedding_provider.vocab
            word_indexes = embedding_provider.word_indexes
            
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ Word vectors: {word_vectors.shape}")
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ Vocabulario: {len(vocab)} palabras")
            log_text.text('\n'.join(logs[-20:]))
            
            # Crear instancia vac√≠a de Top2Vec (m√©todo del notebook)
            model = Top2Vec.__new__(Top2Vec)
            
            # Asignar atributos b√°sicos
            model.documents = np.array(documents, dtype="object")
            model.num_documents = len(documents)
            model.document_ids = np.array([str(i) for i in range(len(documents))])
            model.doc_id2index = dict(zip(model.document_ids, list(range(len(model.document_ids)))))
            model.doc_id_type = np.str_
            model.document_ids_provided = False
            
            # Asignar embeddings precomputados
            model.document_vectors = embedding_provider.embeddings
            model.word_vectors = word_vectors
            model.vocab = vocab
            model.word_indexes = word_indexes
            model.embedding_model = 'precomputed'
            
            # Inicializar variables de indexaci√≥n
            model.topic_index = None
            model.serialized_topic_index = None
            model.topics_indexed = False
            model.document_index = None
            model.serialized_document_index = None
            model.documents_indexed = False
            model.index_id2doc_id = None
            model.doc_id2index_id = None
            model.word_index = None
            model.serialized_word_index = None
            model.words_indexed = False
            model.contextual_top2vec = False
            model.verbose = False
            
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ Modelo base creado")
            log_text.text('\n'.join(logs[-20:]))
            
            # Ejecutar clustering y generaci√≥n de t√≥picos
            progress_bar.progress(50)
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üéØ Ejecutando clustering UMAP + HDBSCAN...")
            log_text.text('\n'.join(logs[-20:]))
            
            model.compute_topics(
                umap_args=umap_args,
                hdbscan_args=hdbscan_args,
                topic_merge_delta=config['topic_merge_delta'],
                gpu_umap=False,
                gpu_hdbscan=False,
                index_topics=False
            )
            
            progress_bar.progress(90)
            
            elapsed = time.time() - training_start
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ Entrenamiento completado en {elapsed/60:.1f} minutos")
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üìä T√≥picos encontrados: {model.get_num_topics()}")
            
            # DEBUG: Verificar estado del modelo
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üîç DEBUG: len(document_vectors) = {len(model.document_vectors)}")
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üîç DEBUG: len(doc_top) = {len(model.doc_top)}")
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üîç DEBUG: document_ids[:5] = {model.document_ids[:5]}")
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üîç DEBUG: document_ids dtype = {model.document_ids.dtype}")
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üîç DEBUG: doc_top[:5] = {model.doc_top[:5]}")
            
            log_text.text('\n'.join(logs[-20:]))
            
            # Paso 5: Guardar modelo
            progress_bar.progress(95)
            status_text.text("Guardando modelo...")
            
            model_dir = Path('modelos') / model_name
            model_dir.mkdir(parents=True, exist_ok=True)
            
            model_path = model_dir / 'modelo.model'
            model.save(str(model_path))
            
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üíæ Modelo guardado: {model_path}")
            log_text.text('\n'.join(logs[-20:]))
            
            # Guardar fechas junto con el modelo
            pub_dates_path = model_dir / 'pub_dates.npy'
            np.save(pub_dates_path, embedding_provider.pub_dates)
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üíæ Fechas guardadas: {pub_dates_path}")
            log_text.text('\n'.join(logs[-20:]))
            
            # Guardar metadata
            total_time = time.time() - start_time
            metadata_path = save_model_metadata(config, model_path, model.get_num_topics(), total_time)
            
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üíæ Metadata guardada: {metadata_path}")
            log_text.text('\n'.join(logs[-20:]))
            
            # Calcular asignaciones de t√≥picos
            progress_bar.progress(97)
            status_text.text("Preparando asignaciones de t√≥picos...")
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üìä Obteniendo asignaciones de t√≥picos...")
            log_text.text('\n'.join(logs[-20:]))
            
            # Usar directamente doc_top y doc_dist que ya fueron calculados por Top2Vec
            num_docs = len(model.document_vectors)
            topic_assignments = model.doc_top  # Ya calculado internamente
            topic_scores_flat = model.doc_dist  # Ya calculado internamente
            
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ Asignaciones calculadas")
            log_text.text('\n'.join(logs[-20:]))
            
            # Generar archivo Excel con resultados completos
            progress_bar.progress(99)
            status_text.text("Generando archivo de resultados...")
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üìÑ Generando Excel con resultados...")
            log_text.text('\n'.join(logs[-20:]))
            
            # Obtener palabras clave de cada t√≥pico
            all_topic_words, all_word_scores, _ = model.get_topics()
            
            # Crear DataFrame con todos los documentos
            results_df = pd.DataFrame({
                'doc_id': range(num_docs),
                'topico': topic_assignments,
                'score_topico': topic_scores_flat,
                'fecha': pd.to_datetime(embedding_provider.pub_dates[:num_docs])
            })
            
            # Agregar palabras clave del t√≥pico asignado (limpias)
            def get_clean_keywords(topic_id):
                words = all_topic_words[topic_id]
                scores = all_word_scores[topic_id]
                clean_w, clean_s = clean_topic_words(words, scores, target_count=10)
                return ', '.join(clean_w)
            
            results_df['palabras_clave'] = results_df['topico'].apply(get_clean_keywords)
            
            # Si hay documentos originales, agregarlos
            if embedding_provider.documents:
                results_df['texto'] = embedding_provider.documents[:num_docs]
            
            # Reordenar columnas
            cols = ['doc_id', 'topico', 'score_topico', 'fecha', 'palabras_clave']
            if 'texto' in results_df.columns:
                cols.append('texto')
            results_df = results_df[cols]
            
            # Guardar Excel
            results_path = model_dir / 'resultados_completos.xlsx'
            with pd.ExcelWriter(results_path, engine='openpyxl') as writer:
                # Hoja 1: Todos los documentos con sus t√≥picos
                results_df.to_excel(writer, sheet_name='Documentos_y_Topicos', index=False)
                
                # Hoja 2: Resumen de t√≥picos
                topic_sizes, topic_nums_sorted = model.get_topic_sizes()
                summary_data = []
                for i, topic_num in enumerate(topic_nums_sorted):
                    words = all_topic_words[i]
                    word_scores = all_word_scores[i]
                    # Limpiar y deduplicar palabras
                    clean_words, clean_scores = clean_topic_words(words, word_scores, target_count=10)
                    summary_data.append({
                        'topico_id': topic_num,
                        'num_documentos': topic_sizes[i],
                        'porcentaje': f"{(topic_sizes[i]/num_docs)*100:.2f}%",
                        'top_10_palabras': ', '.join(clean_words),
                        **{f'palabra_{j+1}': clean_words[j] if j < len(clean_words) else '' for j in range(10)}
                    })
                
                df_summary = pd.DataFrame(summary_data)
                df_summary.to_excel(writer, sheet_name='Resumen_Topicos', index=False)
                
                # Hoja 3: Evoluci√≥n temporal por t√≥pico
                temporal_data = []
                for topic_num in topic_nums_sorted[:20]:  # Top 20 t√≥picos
                    topic_docs = results_df[results_df['topico'] == topic_num]
                    monthly = topic_docs.set_index('fecha').resample('M').size()
                    for date, count in monthly.items():
                        if count > 0:
                            temporal_data.append({
                                'topico': topic_num,
                                'fecha': date,
                                'num_docs': count
                            })
                
                if temporal_data:
                    df_temporal = pd.DataFrame(temporal_data)
                    df_temporal.to_excel(writer, sheet_name='Evolucion_Temporal', index=False)
            
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] üíæ Resultados guardados: {results_path}")
            log_text.text('\n'.join(logs[-20:]))
            
            # Completar
            progress_bar.progress(100)
            status_text.text("‚úÖ Entrenamiento completado!")
            
            # Actualizar m√©tricas finales
            metric_tiempo.metric("Tiempo Total", f"{total_time/60:.1f} min")
            resources = get_system_resources()
            metric_cpu.metric("CPU", f"{resources['cpu_percent']:.1f}%")
            metric_memoria.metric("RAM", f"{resources['memory_percent']:.1f}%")
            metric_eta.metric("T√≥picos", f"{model.get_num_topics()}")
            
            # Guardar en session state
            st.session_state.trained_model = model
            st.session_state.trained_model_data = {
                'name': model_name,
                'path': str(model_path),
                'pub_dates': embedding_provider.pub_dates,
                'metadata': load_model_metadata(model_dir),
                'topic_assignments': topic_assignments,
                'results_path': str(results_path)
            }
            st.session_state.current_model = model
            st.session_state.current_model_data = st.session_state.trained_model_data
            
            # Mensaje de √©xito con ubicaci√≥n de archivos
            st.success(f"""
            ‚úÖ **Modelo entrenado exitosamente!**
            
            - T√≥picos encontrados: {model.get_num_topics()}
            - Documentos procesados: {num_docs:,}
            - Tiempo total: {total_time/60:.1f} minutos
            """)
            
            # Mostrar ubicaci√≥n de archivos generados
            st.info(f"""
            üìÅ **Archivos generados:**
            
            **Modelo:** `{model_path}`
            
            **Resultados completos:** `{results_path}`
            
            El archivo Excel contiene 3 hojas:
            - **Documentos_y_Topicos**: Todos los documentos con su t√≥pico asignado
            - **Resumen_Topicos**: Estad√≠sticas de cada t√≥pico
            - **Evolucion_Temporal**: Evoluci√≥n mensual de los top 20 t√≥picos
            
            üí° Puedes abrir el Excel directamente desde la ubicaci√≥n mostrada.
            """)

            
        except Exception as e:
            progress_bar.progress(0)
            status_text.text("")
            st.error(f"‚ùå Error durante el entrenamiento: {str(e)}")
            logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] ‚ùå ERROR: {str(e)}")
            log_text.text('\n'.join(logs[-20:]))


# =============================================================================
# TAB 2: EXPLORAR RESULTADOS
# =============================================================================

def render_exploration_tab():
    st.markdown("### üìä Exploraci√≥n de Resultados")
    
    # Selector de modelo
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Opci√≥n 1: Usar modelo reci√©n entrenado
        if st.session_state.trained_model is not None:
            use_trained = st.checkbox(
                "üìå Usar modelo reci√©n entrenado",
                value=True,
                help="Explorar el modelo que acabas de entrenar"
            )
            
            if use_trained:
                st.info(f"‚úÖ Modelo activo: **{st.session_state.trained_model_data['name']}**")
                render_topic_explorer(
                    st.session_state.trained_model,
                    st.session_state.trained_model_data
                )
                return
        
        # Opci√≥n 2: Cargar modelo existente
        st.markdown("#### üìÇ Cargar Modelo Existente")
        
        available_models = list_available_models()
        
        if not available_models:
            st.warning("No hay modelos guardados. Entrena un modelo primero en la pesta√±a 'Entrenar Nuevo Modelo'.")
            return
        
        model_options = {
            f"{m['name']} ({m['metadata']['timestamp'][:10]})": m 
            for m in available_models
        }
        
        selected_model_name = st.selectbox(
            "Selecciona un modelo",
            options=list(model_options.keys()),
            help="Modelos guardados ordenados por fecha (m√°s recientes primero)"
        )
        
        selected_model = model_options[selected_model_name]
        
        # Mostrar info del modelo
        with st.expander("‚ÑπÔ∏è Informaci√≥n del Modelo", expanded=True):
            metadata = selected_model['metadata']
            
            col_a, col_b, col_c = st.columns(3)
            
            with col_a:
                st.metric("T√≥picos", metadata['num_topics'])
            with col_b:
                st.metric("Tiempo de Entrenamiento", f"{metadata['execution_time_seconds']/60:.1f} min")
            with col_c:
                st.metric("Fecha", metadata['timestamp'][:10])
            
            st.json(metadata['config'])
        
        # Bot√≥n para cargar
        if st.button("üì• Cargar Modelo", type="primary"):
            with st.spinner("Cargando modelo..."):
                try:
                    model_dir = Path(selected_model['path'])
                    model_path = model_dir / 'modelo.model'
                    model = Top2Vec.load(str(model_path))
                    
                    # Intentar cargar fechas del modelo guardado primero
                    pub_dates_path = model_dir / 'pub_dates.npy'
                    if pub_dates_path.exists():
                        pub_dates = np.load(pub_dates_path)
                    else:
                        # Fallback: cargar del archivo de embeddings original
                        embeddings_file = "data/embeddings_precalculados.npz"
                        if Path(embeddings_file).exists():
                            data = np.load(embeddings_file, allow_pickle=True)
                            # Intentar cargar pub_dates de diferentes fuentes
                            if 'metadata' in data:
                                metadata_npz = data['metadata'].item()
                                pub_dates = metadata_npz.get('pub_date', np.arange(len(model.document_vectors)))
                            elif 'pub_date' in data:
                                pub_dates = data['pub_date']
                            else:
                                pub_dates = np.arange(len(model.document_vectors))
                        else:
                            pub_dates = np.arange(len(model.document_vectors))
                    
                    # Obtener asignaciones de t√≥picos directamente
                    with st.spinner("Cargando asignaciones de t√≥picos..."):
                        # Usar doc_top que ya fue calculado por Top2Vec
                        topic_assignments = model.doc_top
                    
                    st.session_state.current_model = model
                    st.session_state.current_model_data = {
                        'name': selected_model['name'],
                        'path': str(model_path),
                        'pub_dates': pub_dates,
                        'metadata': metadata,
                        'topic_assignments': topic_assignments  # Guardar asignaciones
                    }
                    
                    st.success(f"‚úÖ Modelo '{selected_model['name']}' cargado exitosamente!")
                    st.rerun()
                    
                except Exception as e:
                    st.error(f"‚ùå Error al cargar el modelo: {str(e)}")
    
    # Si hay un modelo cargado, mostrar el explorador
    if st.session_state.current_model is not None:
        st.markdown("---")
        render_topic_explorer(
            st.session_state.current_model,
            st.session_state.current_model_data
        )


def render_topic_explorer(model, model_data):
    """Renderiza el explorador interactivo de t√≥picos"""
    
    st.markdown("### üîç Explorador de T√≥picos")
    
    # Obtener informaci√≥n del modelo
    topic_sizes, topic_nums = model.get_topic_sizes()
    num_topics = len(topic_nums)
    
    # Header con m√©tricas
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total de T√≥picos", num_topics)
    with col2:
        st.metric("Total de Documentos", sum(topic_sizes))
    with col3:
        st.metric("Promedio Docs/T√≥pico", f"{sum(topic_sizes)/num_topics:.0f}")
    with col4:
        st.metric("T√≥pico M√°s Grande", f"{max(topic_sizes):,}")
    
    st.markdown("---")
    
    # Selector de t√≥pico
    topic_options = [
        f"T√≥pico {num} ({topic_sizes[i]:,} docs)" 
        for i, num in enumerate(topic_nums)
    ]
    
    selected_topic_idx = st.selectbox(
        "Selecciona un t√≥pico para explorar",
        range(len(topic_options)),
        format_func=lambda x: topic_options[x]
    )
    
    selected_topic_num = topic_nums[selected_topic_idx]
    
    # Obtener informaci√≥n del t√≥pico
    # Cachear palabras de t√≥picos en session_state para evitar recalcular
    # Usar el path del modelo como identificador √∫nico
    current_model_id = model_data.get('path', id(model))
    
    if ('cached_topic_words' not in st.session_state or 
        'cached_model_id' not in st.session_state or 
        st.session_state.cached_model_id != current_model_id):
        all_topic_words, all_word_scores, _ = model.get_topics()
        st.session_state.cached_topic_words = all_topic_words
        st.session_state.cached_word_scores = all_word_scores
        st.session_state.cached_model_id = current_model_id
    
    words = st.session_state.cached_topic_words[selected_topic_num]
    word_scores = st.session_state.cached_word_scores[selected_topic_num]
    
    # Limpiar y deduplicar palabras (eliminar variantes con puntuaci√≥n)
    top_words, top_scores = clean_topic_words(words, word_scores, target_count=20)
    
    # Layout en dos columnas (m√°s ancho)
    col_left, col_right = st.columns([1.2, 1.8])
    
    with col_left:
        st.markdown(f"#### ‚òÅÔ∏è WordCloud - T√≥pico {selected_topic_num}")
        
        # Generar wordcloud
        try:
            wc_image = create_wordcloud_image(top_words, top_scores, width=800, height=500)
            st.image(wc_image, use_container_width=True)
            
            # Bot√≥n de descarga del WordCloud
            st.download_button(
                label="üíæ Descargar WordCloud (PNG)",
                data=wc_image,
                file_name=f"wordcloud_topico_{selected_topic_num}.png",
                mime="image/png",
                use_container_width=True
            )
        except Exception as e:
            st.error(f"Error generando wordcloud: {e}")
        
        # Tabla de palabras clave
        st.markdown("##### üìù Palabras Clave")
        
        df_words = pd.DataFrame({
            'Palabra': top_words,
            'Relevancia': [f"{score:.4f}" for score in top_scores]
        })
        
        st.dataframe(df_words, use_container_width=True, height=400)
    
    with col_right:
        st.markdown(f"#### üìà Evoluci√≥n Temporal - T√≥pico {selected_topic_num}")
        
        # Selector de frecuencia de agrupaci√≥n
        freq_option = st.selectbox(
            "Frecuencia de agrupaci√≥n",
            ["Diaria", "Semanal", "Mensual", "Trimestral", "Anual"],
            index=2,  # Mensual por defecto
            key=f"freq_{selected_topic_num}"
        )
        
        freq_map = {
            "Diaria": "D",
            "Semanal": "W",
            "Mensual": "M",
            "Trimestral": "Q",
            "Anual": "Y"
        }
        
        # Crear serie temporal
        try:
            # Usar las asignaciones de t√≥picos pre-calculadas
            if 'topic_assignments' not in model_data:
                st.error("Las asignaciones de t√≥picos no est√°n disponibles. Por favor, recarga el modelo.")
                return
            
            topic_assignments = model_data['topic_assignments']
            pub_dates_subset = model_data['pub_dates'][:len(topic_assignments)]
            
            # Convertir fechas a datetime
            fechas = pd.to_datetime(pub_dates_subset)
            
            df_temporal = pd.DataFrame({
                'fecha': fechas,
                'topico': topic_assignments
            })
            
            # Filtrar por t√≥pico seleccionado
            df_topic = df_temporal[df_temporal['topico'] == selected_topic_num].copy()
            
            if len(df_topic) == 0:
                st.warning(f"No hay documentos en el t√≥pico {selected_topic_num}")
            else:
                # Agrupar por la frecuencia seleccionada
                df_topic = df_topic.set_index('fecha')
                conteo = df_topic.resample(freq_map[freq_option]).size().reset_index(name='frecuencia')
                conteo.columns = ['fecha', 'frecuencia']
                
                # Filtrar fechas con frecuencia > 0
                conteo = conteo[conteo['frecuencia'] > 0]
                
                # Gr√°fico interactivo
                fig = go.Figure()
                
                fig.add_trace(go.Scatter(
                    x=conteo['fecha'],
                    y=conteo['frecuencia'],
                    mode='lines+markers',
                    name=f'T√≥pico {selected_topic_num}',
                    line=dict(color='#1f77b4', width=2),
                    marker=dict(size=6),
                    hovertemplate='<b>Fecha:</b> %{x|%Y-%m-%d}<br><b>Documentos:</b> %{y}<extra></extra>'
                ))
                
                fig.update_layout(
                    title=f"Frecuencia {freq_option} de Documentos - T√≥pico {selected_topic_num}",
                    xaxis_title="Fecha",
                    yaxis_title="N√∫mero de Documentos",
                    hovermode='x unified',
                    height=400
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Bot√≥n para descargar gr√°fico
                fig_bytes = fig.to_image(format="png", width=1200, height=600)
                st.download_button(
                    label="üíæ Descargar Gr√°fico (PNG)",
                    data=fig_bytes,
                    file_name=f"evolucion_temporal_topico_{selected_topic_num}_{freq_option.lower()}.png",
                    mime="image/png",
                    use_container_width=True
                )
                
                # Estad√≠sticas temporales
                st.markdown("##### üìä Estad√≠sticas Temporales")
                
                col_a, col_b, col_c, col_d = st.columns(4)
                
                with col_a:
                    st.metric("Total Docs", len(df_topic))
                with col_b:
                    st.metric(f"Promedio {freq_option}", f"{conteo['frecuencia'].mean():.1f}")
                with col_c:
                    st.metric(f"M√°ximo {freq_option}", conteo['frecuencia'].max())
                with col_d:
                    fecha_min = df_topic.index.min().strftime('%Y-%m-%d')
                    fecha_max = df_topic.index.max().strftime('%Y-%m-%d')
                    st.metric("Rango", f"{fecha_min} a {fecha_max}")
            
        except Exception as e:
            st.error(f"Error generando serie temporal: {e}")
    
    # Secci√≥n de an√°lisis adicionales
    st.markdown("---")
    st.markdown("### üìë An√°lisis Adicionales")
    
    with st.expander("üìÑ Ver Documentos Representativos", expanded=False):
        try:
            documents, document_scores, document_ids = model.search_documents_by_topic(
                topic_num=selected_topic_num,
                num_docs=5
            )
            
            for i, (doc, score, doc_id) in enumerate(zip(documents, document_scores, document_ids), 1):
                st.markdown(f"**Documento {i}** (ID: {doc_id}, Relevancia: {score:.3f})")
                st.text_area(
                    label="",
                    value=doc[:500] + "..." if len(doc) > 500 else doc,
                    height=100,
                    key=f"doc_{i}",
                    label_visibility="collapsed"
                )
        except Exception as e:
            st.error(f"Error obteniendo documentos: {e}")
    
    with st.expander("üìä Distribuci√≥n de Documentos", expanded=False):
        try:
            # Gr√°fico de barras de todos los t√≥picos
            df_distribution = pd.DataFrame({
                'T√≥pico': [f"T√≥pico {num}" for num in topic_nums],
                'Documentos': topic_sizes
            })
            
            fig_dist = px.bar(
                df_distribution,
                x='T√≥pico',
                y='Documentos',
                title="Distribuci√≥n de Documentos por T√≥pico",
                color='Documentos',
                color_continuous_scale='viridis'
            )
            
            fig_dist.update_layout(height=400)
            st.plotly_chart(fig_dist, use_container_width=True)
            
        except Exception as e:
            st.error(f"Error generando distribuci√≥n: {e}")
    
    # Secci√≥n de archivo de resultados
    st.markdown("---")
    st.markdown("### üìä Resultados Completos")
    
    # Determinar la ruta del archivo de resultados
    if 'results_path' in model_data:
        results_path = Path(model_data['results_path'])
    else:
        # Construir ruta esperada basada en el path del modelo
        model_dir = Path(model_data['path']).parent
        results_path = model_dir / 'resultados_completos.xlsx'
    
    # Verificar si existe el archivo
    if results_path.exists():
        st.info(f"""
        üìÅ **Archivo de resultados completos generado autom√°ticamente:**
        
        `{results_path}`
        
        El archivo Excel contiene 3 hojas:
        - **Documentos_y_Topicos**: Lista completa de todos los documentos con su t√≥pico asignado, fecha, score, y palabras clave
        - **Resumen_Topicos**: Resumen estad√≠stico de cada t√≥pico
        - **Evolucion_Temporal**: Evoluci√≥n mensual de los top 20 t√≥picos
        """)
        
        # Bot√≥n para descargar el archivo de resultados
        with open(results_path, 'rb') as f:
            st.download_button(
                label="üì• Descargar Resultados Completos (Excel)",
                data=f.read(),
                file_name=f"resultados_{model_data['name']}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True,
                type="primary"
            )
    else:
        st.info(f"""
        üìÅ **El archivo con los datos completos est√° en la ruta:**
        
        `{results_path}`
        
        *Nota: Si el archivo no existe a√∫n, se generar√° autom√°ticamente cuando vuelvas a entrenar un modelo.*
        """)


# =============================================================================
# TAB 3: AYUDA Y DOCUMENTACI√ìN
# =============================================================================

def render_help_tab():
    """Renderiza la pesta√±a de ayuda y documentaci√≥n dentro de la app"""
    
    st.markdown("## üìñ Ayuda y Documentaci√≥n")
    
    # Subtabs para organizar la documentaci√≥n
    help_tab1, help_tab2, help_tab3, help_tab4, help_tab5, help_tab6 = st.tabs([
        "üìö Fundamentos Top2Vec",
        "üöÄ Inicio R√°pido",
        "üìä Datos y Configuraci√≥n",
        "üéØ C√≥mo Usar",
        "‚ùì Preguntas Frecuentes",
        "üîß Soluci√≥n de Problemas"
    ])
    
    with help_tab1:
        render_top2vec_fundamentals()
    
    with help_tab2:
        render_quick_start_help()
    
    with help_tab3:
        render_data_info_help()
    
    with help_tab4:
        render_usage_help()
    
    with help_tab5:
        render_faq_help()
    
    with help_tab6:
        render_troubleshooting_help()


def render_top2vec_fundamentals():
    """Fundamentos te√≥ricos y metodol√≥gicos de Top2Vec"""
    st.markdown("### üìö Fundamentos de Top2Vec")
    
    st.markdown("""
    <div class="info-card">
    <strong>üìñ Fundamentaci√≥n Cient√≠fica</strong><br>
    Top2Vec es un algoritmo de modelado de t√≥picos basado en embeddings densos y clustering 
    no supervisado, publicado en 2020 por Dimo Angelov.
    </div>
    """, unsafe_allow_html=True)
    
    # Sub-tabs para organizar teor√≠a
    theory_tab1, theory_tab2, theory_tab3, theory_tab4, theory_tab5 = st.tabs([
        "üß† ¬øQu√© es Top2Vec?",
        "‚öôÔ∏è C√≥mo Funciona",
        "üìä Comparaci√≥n con Otros M√©todos",
        "üìñ Referencias Cient√≠ficas",
        "üéì Interpretaci√≥n de Resultados"
    ])
    
    with theory_tab1:
        st.markdown("""
        #### üß† ¬øQu√© es Top2Vec?
        
        **Top2Vec** (Topic-to-Vector) es un m√©todo autom√°tico de descubrimiento de t√≥picos que utiliza 
        representaciones vectoriales densas (embeddings) de documentos y palabras.
        
        ---
        
        ##### üéØ Objetivo Principal
        
        Identificar autom√°ticamente **t√≥picos sem√°nticamente coherentes** en colecciones grandes de documentos 
        sin necesidad de:
        - Especificar el n√∫mero de t√≥picos *a priori*
        - Preprocesamiento intensivo (lematizaci√≥n, stemming)
        - Diccionarios o vocabularios predefinidos
        
        ---
        
        ##### üîë Conceptos Clave
        
        **1. Embeddings de Documentos**
        - Cada documento se representa como un vector denso en espacio de alta dimensi√≥n (t√≠picamente 300 dims)
        - Documentos similares tienen vectores cercanos
        - Capturan significado sem√°ntico, no solo co-ocurrencia de palabras
        
        **2. Embeddings de Palabras**
        - Cada palabra tiene su propio vector
        - Palabras con significados similares est√°n cerca en el espacio vectorial
        - Ejemplo: "banco", "entidad", "financiera" estar√°n cercanas
        
        **3. Clustering Autom√°tico**
        - Los documentos se agrupan autom√°ticamente por similitud sem√°ntica
        - Cada cluster = un t√≥pico
        - No necesitas decir cu√°ntos t√≥picos quieres
        
        **4. Vectores de T√≥picos**
        - Cada t√≥pico tiene un centroide (promedio de documentos del cluster)
        - Las palabras m√°s cercanas al centroide son las "palabras clave" del t√≥pico
        
        ---
        
        ##### üí° Diferencia Clave vs M√©todos Cl√°sicos
        
        | Aspecto | Top2Vec | LDA (Cl√°sico) |
        |---------|---------|---------------|
        | **Representaci√≥n** | Vectores densos (embeddings) | Bolsa de palabras (sparse) |
        | **Sem√°ntica** | ‚úÖ Captura significado | ‚ùå Solo co-ocurrencia |
        | **N√∫m. t√≥picos** | ‚úÖ Autom√°tico | ‚ùå Debes especificarlo |
        | **Preprocesamiento** | ‚ö†Ô∏è M√≠nimo | ‚ö†Ô∏è Intensivo |
        | **Textos cortos** | ‚úÖ Funciona bien | ‚ùå Problemas |
        
        ---
        
        ##### üéì Base Te√≥rica
        
        Top2Vec se fundamenta en tres √°reas de NLP moderno:
        
        1. **Word Embeddings (Word2Vec, 2013)**
           - Mikolov et al.: Representaciones vectoriales de palabras
           - Base: "palabras en contextos similares tienen significados similares"
        
        2. **Document Embeddings (Doc2Vec, 2014)**
           - Le & Mikolov: Extensi√≥n de Word2Vec para documentos completos
           - Cada documento = vector en mismo espacio que palabras
        
        3. **Density-Based Clustering (HDBSCAN, 2015)**
           - Campello et al.: Clustering jer√°rquico basado en densidad
           - Detecta autom√°ticamente n√∫mero de clusters
           - Robusto a ruido y outliers
        
        ---
        
        ##### üìê Intuici√≥n Geom√©trica
        
        Imagina un espacio 3D donde:
        - Cada punto = un documento
        - Documentos sobre "inflaci√≥n" est√°n en una regi√≥n
        - Documentos sobre "empleo" en otra regi√≥n
        - Documentos sobre "pol√≠tica monetaria" en otra
        
        Top2Vec:
        1. Encuentra estas regiones densas (clusters)
        2. Calcula el "centro" de cada regi√≥n
        3. Encuentra las palabras m√°s cercanas a cada centro
        4. ¬°Esas palabras definen el t√≥pico!
        """)
        
        # Diagrama conceptual (texto ASCII)
        st.markdown("""
        **Diagrama Conceptual:**
        ```
        Espacio de Embeddings (simplificado en 2D):
        
                T√≥pico 1: "Inflaci√≥n"
                    ‚óè‚óè‚óè‚óè‚óè
                   ‚óè  ‚äó  ‚óè  ‚Üê Centroide
                    ‚óè‚óè‚óè‚óè‚óè
                    
        T√≥pico 2: "Empleo"           T√≥pico 3: "BCE"
           ‚óè‚óè‚óè                          ‚óè‚óè‚óè‚óè
          ‚óè  ‚äó ‚óè                        ‚óè ‚äó ‚óè
           ‚óè‚óè‚óè                          ‚óè‚óè‚óè‚óè
        
        ‚óè = Documento
        ‚äó = Centroide del t√≥pico
        
        Palabras cercanas al centroide = Palabras clave del t√≥pico
        ```
        """)
    
    with theory_tab2:
        st.markdown("""
        #### ‚öôÔ∏è C√≥mo Funciona Top2Vec - Pipeline Completo
        
        Top2Vec ejecuta 5 pasos principales:
        
        ---
        
        ##### **PASO 1: Crear Embeddings de Documentos** üìù
        
        **Input**: Texto crudo de documentos
        
        **Proceso**:
        - Tokenizaci√≥n b√°sica (separar palabras)
        - Entrenar modelo Doc2Vec o usar embeddings precalculados
        - Cada documento ‚Üí vector de 300 dimensiones
        
        **Output**: Matriz de documentos (N √ó 300)
        - N = n√∫mero de documentos
        - 300 = dimensiones del embedding
        
        **En esta app**: Usamos embeddings **precalculados** para ahorrar 2-3 horas
        
        ```python
        # Conceptualmente:
        doc1 = "El BCE sube los tipos de inter√©s"
        embedding1 = [0.23, -0.45, 0.12, ..., 0.67]  # 300 valores
        
        doc2 = "El BCE mantiene pol√≠tica monetaria"
        embedding2 = [0.21, -0.43, 0.15, ..., 0.65]  # Similar a doc1
        
        doc3 = "El desempleo juvenil aumenta"
        embedding3 = [-0.55, 0.32, -0.78, ..., 0.12]  # Diferente
        ```
        
        ---
        
        ##### **PASO 2: Reducci√≥n de Dimensionalidad (UMAP)** üîª
        
        **Problema**: 300 dimensiones son demasiadas para clustering eficiente
        
        **Soluci√≥n**: UMAP (Uniform Manifold Approximation and Projection)
        
        **Proceso**:
        - Reduce de 300D ‚Üí 5D (t√≠picamente)
        - Preserva estructura local y global
        - Mantiene relaciones de similitud
        
        **Par√°metros clave**:
        - `n_neighbors`: Cu√°ntos vecinos considerar
          - Mayor = estructura m√°s global
          - Menor = estructura m√°s local
        - `n_components`: Dimensiones finales (t√≠picamente 5)
        - `metric`: Distancia a usar (t√≠picamente 'cosine')
        
        **Output**: Matriz reducida (N √ó 5)
        
        ```python
        # Antes (300D):
        doc1_high = [0.23, -0.45, 0.12, ..., 0.67]  # 300 valores
        
        # Despu√©s (5D):
        doc1_low = [1.2, -0.5, 3.4, 0.8, -2.1]  # 5 valores
        ```
        
        **¬øPor qu√© UMAP y no PCA?**
        - ‚úÖ UMAP preserva mejor estructura no-lineal
        - ‚úÖ Mejor para visualizaci√≥n
        - ‚úÖ Mantiene clusters locales
        - ‚ùå PCA solo captura varianza lineal
        
        ---
        
        ##### **PASO 3: Clustering con HDBSCAN** üéØ
        
        **Objetivo**: Agrupar documentos similares autom√°ticamente
        
        **Algoritmo**: HDBSCAN (Hierarchical Density-Based Spatial Clustering)
        
        **¬øC√≥mo funciona?**
        1. Estima densidad local de puntos
        2. Identifica regiones de alta densidad
        3. Agrupa puntos en esas regiones
        4. Puntos en regiones de baja densidad = ruido (outliers)
        
        **Par√°metros clave**:
        - `min_cluster_size`: Tama√±o m√≠nimo de un cluster v√°lido
          - Mayor = menos t√≥picos, m√°s generales
          - Menor = m√°s t√≥picos, m√°s espec√≠ficos
        - `min_samples`: Cu√°n conservador ser con outliers
        
        **Output**: Etiquetas de cluster para cada documento
        ```python
        doc1 ‚Üí T√≥pico 0 (Pol√≠tica monetaria)
        doc2 ‚Üí T√≥pico 0 (Pol√≠tica monetaria)
        doc3 ‚Üí T√≥pico 5 (Empleo)
        doc4 ‚Üí -1 (Ruido/Outlier)
        ```
        
        **Ventaja vs K-Means**:
        - ‚úÖ Detecta autom√°ticamente n√∫mero de clusters
        - ‚úÖ Maneja clusters de formas irregulares
        - ‚úÖ Identifica outliers
        - ‚úÖ No asume clusters esf√©ricos
        
        ---
        
        ##### **PASO 4: Calcular Vectores de T√≥picos** üìä
        
        **Para cada cluster/t√≥pico**:
        
        1. Tomar todos los embeddings de documentos del cluster
        2. Calcular el centroide (promedio)
        
        ```python
        # T√≥pico 0 tiene 150 documentos
        docs_topic0 = [embedding1, embedding2, ..., embedding150]
        
        # Centroide = promedio
        topic_vector0 = mean(docs_topic0)
        ```
        
        **Este vector representa el "tema central" del t√≥pico**
        
        ---
        
        ##### **PASO 5: Encontrar Palabras del T√≥pico** üî§
        
        **Objetivo**: Identificar palabras que mejor describen cada t√≥pico
        
        **Proceso**:
        1. Cargar embeddings de palabras (vocabulario)
        2. Para cada t√≥pico, calcular similitud coseno entre:
           - Vector del t√≥pico
           - Cada palabra del vocabulario
        3. Ordenar palabras por similitud
        4. Top N palabras = palabras clave del t√≥pico
        
        ```python
        # Vector del t√≥pico 0
        topic0_vector = [0.22, -0.44, 0.13, ..., 0.66]
        
        # Palabras del vocabulario
        word_vectors = {
            "inflaci√≥n": [0.21, -0.43, 0.12, ..., 0.65],
            "tipos": [0.23, -0.45, 0.14, ..., 0.67],
            "inter√©s": [0.20, -0.42, 0.11, ..., 0.64],
            ...
        }
        
        # Calcular similitudes
        similitudes = {
            "inflaci√≥n": cosine_similarity(topic0_vector, word_vectors["inflaci√≥n"]),
            "tipos": cosine_similarity(topic0_vector, word_vectors["tipos"]),
            ...
        }
        
        # Top 10 palabras
        top_words = ["inflaci√≥n", "tipos", "inter√©s", "precios", "IPC", ...]
        ```
        
        ---
        
        ##### **üîÑ Pipeline Completo Resumido**
        
        ```
        1. DOCUMENTOS CRUDOS
           ‚Üì
        2. DOC2VEC ‚Üí EMBEDDINGS (N √ó 300)
           ‚Üì
        3. UMAP ‚Üí REDUCCI√ìN (N √ó 5)
           ‚Üì
        4. HDBSCAN ‚Üí CLUSTERS (etiquetas)
           ‚Üì
        5. CENTROIDES ‚Üí VECTORES DE T√ìPICOS
           ‚Üì
        6. SIMILITUD COSENO ‚Üí PALABRAS CLAVE
           ‚Üì
        7. T√ìPICOS FINALES ‚úÖ
        ```
        
        ---
        
        ##### ‚è±Ô∏è Tiempo de Cada Paso (50K documentos)
        
        | Paso | Tiempo | Notas |
        |------|--------|-------|
        | 1. Doc2Vec | 2-3 horas | **Precalculado en esta app** |
        | 2. UMAP | 5-10 min | Depende de n_neighbors |
        | 3. HDBSCAN | 3-5 min | Depende de min_cluster_size |
        | 4. Centroides | <1 min | C√°lculo simple |
        | 5. Palabras | 1-2 min | B√∫squeda de similitud |
        | **Total** | **~15-20 min** | **Sin Doc2Vec** |
        
        """)
    
    with theory_tab3:
        st.markdown("""
        #### üìä Comparaci√≥n con Otros M√©todos de Topic Modeling
        
        ---
        
        ##### üÜö Top2Vec vs LDA (Latent Dirichlet Allocation)
        
        | Caracter√≠stica | Top2Vec | LDA |
        |----------------|---------|-----|
        | **A√±o** | 2020 | 2003 |
        | **Representaci√≥n** | Embeddings densos (Doc2Vec) | Bolsa de palabras (sparse) |
        | **Sem√°ntica** | ‚úÖ Captura significado contextual | ‚ùå Solo co-ocurrencia |
        | **N√∫m. t√≥picos** | ‚úÖ Autom√°tico (HDBSCAN) | ‚ùå Debes especificar K |
        | **Preprocesamiento** | ‚ö†Ô∏è M√≠nimo (tokenizaci√≥n) | ‚ö†Ô∏è Intensivo (stopwords, lemma, stem) |
        | **Textos cortos** | ‚úÖ Funciona bien | ‚ùå Problemas (poca co-ocurrencia) |
        | **Interpretabilidad** | ‚úÖ Palabras por similitud vectorial | ‚úÖ Probabilidades claras |
        | **Velocidad entrenamiento** | ‚ö†Ô∏è Medio (si Doc2Vec ya existe) | ‚úÖ R√°pido |
        | **Escalabilidad** | ‚ö†Ô∏è Medio (UMAP puede ser lento) | ‚úÖ Buena (Online LDA) |
        | **Outliers** | ‚úÖ Detecta y maneja | ‚ùå Asigna a alg√∫n t√≥pico |
        | **T√≥picos overlapping** | ‚ùå Un doc = un t√≥pico | ‚úÖ Distribuci√≥n de t√≥picos |
        
        **¬øCu√°ndo usar Top2Vec?**
        - Corpus con textos cortos (tweets, noticias, reviews)
        - No sabes cu√°ntos t√≥picos esperar
        - Quieres capturar significado sem√°ntico
        - Tienes recursos computacionales razonables
        
        **¬øCu√°ndo usar LDA?**
        - Corpus muy grande (millones de docs)
        - Sabes aproximadamente cu√°ntos t√≥picos quieres
        - Necesitas interpretabilidad probabil√≠stica
        - Quieres actualizar modelo incrementalmente
        
        ---
        
        ##### üÜö Top2Vec vs BERTopic
        
        | Caracter√≠stica | Top2Vec | BERTopic |
        |----------------|---------|----------|
        | **A√±o** | 2020 | 2020 |
        | **Embeddings** | Doc2Vec (300D) | BERT/Sentence-BERT (768D) |
        | **Calidad sem√°ntica** | ‚úÖ Buena | ‚úÖ‚úÖ Excelente |
        | **Clustering** | HDBSCAN | HDBSCAN |
        | **Representaci√≥n** | Palabras cercanas | c-TF-IDF |
        | **Velocidad** | ‚úÖ R√°pido | ‚ö†Ô∏è M√°s lento (BERT pesado) |
        | **Recursos** | üíª Moderados | üíªüíª Altos (GPU recomendada) |
        | **Idiomas** | ‚ö†Ô∏è Necesita modelo por idioma | ‚úÖ Multiling√ºe (BERT multiling√ºe) |
        | **Actualizaci√≥n temporal** | ‚ö†Ô∏è Re-entrenar | ‚úÖ Mejor soporte |
        
        **¬øCu√°ndo usar Top2Vec?**
        - Recursos limitados (sin GPU)
        - Velocidad es importante
        - Embeddings Doc2Vec son suficientes
        
        **¬øCu√°ndo usar BERTopic?**
        - M√°xima calidad sem√°ntica
        - Tienes GPU
        - Corpus multiling√ºe
        - Necesitas din√°micas temporales avanzadas
        
        ---
        
        ##### üÜö Top2Vec vs NMF (Non-Negative Matrix Factorization)
        
        | Caracter√≠stica | Top2Vec | NMF |
        |----------------|---------|-----|
        | **Representaci√≥n** | Embeddings densos | TF-IDF sparse |
        | **Sem√°ntica** | ‚úÖ S√≠ | ‚ùå No |
        | **N√∫m. t√≥picos** | ‚úÖ Autom√°tico | ‚ùå Manual |
        | **Interpretabilidad** | ‚úÖ Buena | ‚úÖ‚úÖ Excelente |
        | **Velocidad** | ‚ö†Ô∏è Medio | ‚úÖ R√°pido |
        | **T√≥picos disjuntos** | ‚úÖ S√≠ | ‚ö†Ô∏è Parcial |
        
        ---
        
        ##### üìà Comparaci√≥n Visual de Calidad
        
        **Coherencia de T√≥picos** (mayor = mejor):
        ```
        BERTopic     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 95%
        Top2Vec      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë 85%
        LDA          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 70%
        NMF          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 65%
        ```
        
        **Velocidad** (menor = m√°s r√°pido):
        ```
        NMF          ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Muy r√°pido
        LDA          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë R√°pido
        Top2Vec      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Medio
        BERTopic     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà Lento
        ```
        
        **Requerimientos Computacionales**:
        ```
        NMF          üíª Bajo
        LDA          üíª Bajo
        Top2Vec      üíªüíª Medio
        BERTopic     üíªüíªüíª Alto
        ```
        
        ---
        
        ##### üéØ Resumen: ¬øCu√°l Elegir?
        
        **Top2Vec es ideal cuando**:
        - ‚úÖ Necesitas descubrir t√≥picos autom√°ticamente
        - ‚úÖ Corpus tiene textos cortos o medianos
        - ‚úÖ Calidad sem√°ntica es importante
        - ‚úÖ Tienes recursos computacionales moderados
        - ‚úÖ No quieres preprocesamiento intensivo
        
        **En nuestro caso (noticias econ√≥micas)**:
        - ‚úÖ Textos moderadamente largos
        - ‚úÖ No sabemos cu√°ntos t√≥picos hay
        - ‚úÖ Queremos capturar significado (BCE = Banco Central Europeo)
        - ‚úÖ Recursos disponibles: CPU est√°ndar, 8-16 GB RAM
        
        ‚Üí **Top2Vec es la elecci√≥n correcta** ‚úÖ
        """)
    
    with theory_tab4:
        st.markdown("""
        #### üìñ Referencias Cient√≠ficas y Bibliograf√≠a
        
        ---
        
        ##### üìÑ Paper Original de Top2Vec
        
        **T√≠tulo**: *Top2Vec: Distributed Representations of Topics*
        
        **Autor**: Dimo Angelov
        
        **A√±o**: 2020
        
        **Publicaci√≥n**: arXiv preprint arXiv:2008.09470
        
        **Abstract**: 
        > "We present Top2Vec, an algorithm for topic modeling and semantic search. 
        > It automatically detects topics present in text and generates jointly 
        > embedded topic, document and word vectors."
        
        **Enlace**: https://arxiv.org/abs/2008.09470
        
        **Cita (BibTeX)**:
        ```bibtex
        @article{angelov2020top2vec,
          title={Top2Vec: Distributed Representations of Topics},
          author={Angelov, Dimo},
          journal={arXiv preprint arXiv:2008.09470},
          year={2020}
        }
        ```
        
        ---
        
        ##### üìö Fundamentos Te√≥ricos
        
        **1. Word2Vec (2013)**
        
        - **Paper**: *Efficient Estimation of Word Representations in Vector Space*
        - **Autores**: Mikolov, T., Chen, K., Corrado, G., & Dean, J.
        - **Publicaci√≥n**: ICLR 2013
        - **Enlace**: https://arxiv.org/abs/1301.3781
        - **Contribuci√≥n**: Embeddings de palabras usando CBOW y Skip-gram
        
        **2. Doc2Vec (2014)**
        
        - **Paper**: *Distributed Representations of Sentences and Documents*
        - **Autores**: Le, Q., & Mikolov, T.
        - **Publicaci√≥n**: ICML 2014
        - **Enlace**: https://arxiv.org/abs/1405.4053
        - **Contribuci√≥n**: Extensi√≥n de Word2Vec para documentos completos
        
        **3. UMAP (2018)**
        
        - **Paper**: *UMAP: Uniform Manifold Approximation and Projection*
        - **Autores**: McInnes, L., Healy, J., & Melville, J.
        - **Publicaci√≥n**: arXiv preprint arXiv:1802.03426
        - **Enlace**: https://arxiv.org/abs/1802.03426
        - **Contribuci√≥n**: Reducci√≥n de dimensionalidad preservando estructura topol√≥gica
        
        **4. HDBSCAN (2015)**
        
        - **Paper**: *Density-Based Clustering Based on Hierarchical Density Estimates*
        - **Autores**: Campello, R. J., Moulavi, D., & Sander, J.
        - **Publicaci√≥n**: PAKDD 2013
        - **DOI**: 10.1007/978-3-642-37456-2_14
        - **Contribuci√≥n**: Clustering jer√°rquico basado en densidad con detecci√≥n autom√°tica
        
        ---
        
        ##### üìä Comparaciones con Otros M√©todos
        
        **LDA (2003)**
        
        - **Paper**: *Latent Dirichlet Allocation*
        - **Autores**: Blei, D. M., Ng, A. Y., & Jordan, M. I.
        - **Publicaci√≥n**: JMLR 2003
        - **Enlace**: https://www.jmlr.org/papers/v3/blei03a.html
        
        **BERTopic (2022)**
        
        - **Paper**: *BERTopic: Neural topic modeling with a class-based TF-IDF procedure*
        - **Autor**: Grootendorst, M.
        - **Publicaci√≥n**: arXiv preprint arXiv:2203.05794
        - **Enlace**: https://arxiv.org/abs/2203.05794
        
        ---
        
        ##### üî¨ Validaci√≥n y Evaluaci√≥n
        
        **M√©tricas de Coherencia**
        
        - **Paper**: *Exploring the Space of Topic Coherence Measures*
        - **Autores**: R√∂der, M., Both, A., & Hinneburg, A.
        - **Publicaci√≥n**: WSDM 2015
        - **Contribuci√≥n**: C_v, C_uci, C_npmi para evaluar calidad de t√≥picos
        
        ---
        
        ##### üìñ Libros de Referencia
        
        **1. Natural Language Processing with Python**
        - Autores: Bird, S., Klein, E., & Loper, E.
        - Editorial: O'Reilly (2009)
        - Cap√≠tulos relevantes: 6 (Clasificaci√≥n), 7 (Estructura)
        
        **2. Speech and Language Processing**
        - Autores: Jurafsky, D., & Martin, J. H.
        - Editorial: Pearson (3rd ed., 2023)
        - Cap√≠tulos: 6 (Vector Semantics), 25 (Topic Models)
        
        **3. Introduction to Information Retrieval**
        - Autores: Manning, C. D., Raghavan, P., & Sch√ºtze, H.
        - Editorial: Cambridge (2008)
        - Cap√≠tulos: 18 (Matrix decompositions), 19 (Latent Semantic Indexing)
        
        ---
        
        ##### üåê Recursos Online
        
        **Documentaci√≥n Oficial**:
        - Top2Vec GitHub: https://github.com/ddangelov/Top2Vec
        - UMAP Docs: https://umap-learn.readthedocs.io/
        - HDBSCAN Docs: https://hdbscan.readthedocs.io/
        
        **Tutoriales**:
        - Gensim Doc2Vec: https://radimrehurek.com/gensim/models/doc2vec.html
        - UMAP Beginner Tutorial: https://umap-learn.readthedocs.io/en/latest/basic_usage.html
        
        **Datasets P√∫blicos**:
        - 20 Newsgroups: http://qwone.com/~jason/20Newsgroups/
        - Reuters Corpus: https://martin-thoma.com/nlp-reuters/
        
        ---
        
        ##### üìä Aplicaciones en Econom√≠a
        
        **1. Topic Modeling in Economics**
        - Autores: Hansen, S., McMahon, M., & Prat, A.
        - Publicaci√≥n: AEJ: Macroeconomics (2018)
        - Tema: Comunicaci√≥n de bancos centrales
        
        **2. Central Bank Communication and Policy Effectiveness**
        - Autores: Ehrmann, M., & Fratzscher, M.
        - Publicaci√≥n: ECB Working Paper (2007)
        - Tema: An√°lisis de comunicaciones del BCE
        
        **3. News and Narratives in Financial Systems**
        - Autores: Shiller, R. J.
        - Publicaci√≥n: American Economic Review (2017)
        - Tema: Narrativas econ√≥micas en medios
        """)
    
    with theory_tab5:
        st.markdown("""
        #### üéì Interpretaci√≥n de Resultados
        
        ---
        
        ##### üìä Entender los T√≥picos Encontrados
        
        **Un t√≥pico en Top2Vec NO es**:
        - ‚ùå Una categor√≠a predefinida
        - ‚ùå Un tema √∫nico y absoluto
        - ‚ùå Una asignaci√≥n determinista
        
        **Un t√≥pico en Top2Vec S√ç es**:
        - ‚úÖ Un cluster de documentos sem√°nticamente similares
        - ‚úÖ Una distribuci√≥n de palabras relacionadas
        - ‚úÖ Una representaci√≥n emergente del corpus
        
        ---
        
        ##### üî§ Interpretar Palabras Clave
        
        **Ejemplo de T√≥pico**:
        ```
        T√≥pico 5: 
        Palabras: ["inflaci√≥n", "precios", "IPC", "subida", "tasa", 
                   "consumo", "datos", "interanual", "energ√≠a", "subyacente"]
        ```
        
        **¬øC√≥mo interpretar?**
        
        1. **Lee las primeras 5-10 palabras**: Dan la "esencia" del t√≥pico
        2. **Busca coherencia tem√°tica**: ¬øHablan del mismo tema?
        3. **Identifica el concepto central**: En este caso ‚Üí "Inflaci√≥n y precios"
        4. **Verifica con documentos representativos**: ¬øConfirman tu interpretaci√≥n?
        
        **Se√±ales de un t√≥pico coherente**:
        - ‚úÖ Palabras claramente relacionadas
        - ‚úÖ Campo sem√°ntico unificado
        - ‚úÖ Documentos representativos confirman interpretaci√≥n
        
        **Se√±ales de un t√≥pico problem√°tico**:
        - ‚ö†Ô∏è Palabras muy gen√©ricas ("datos", "seg√∫n", "puede")
        - ‚ö†Ô∏è Mezcla de temas no relacionados
        - ‚ö†Ô∏è Pocos documentos asignados
        
        ---
        
        ##### üìà Interpretar el N√∫mero de T√≥picos
        
        **¬øCu√°ntos t√≥picos son "buenos"?**
        
        No hay n√∫mero m√°gico, depende del corpus:
        
        - **Corpus muy homog√©neo** (ej: solo sobre BCE)
          - Espera: 10-30 t√≥picos
          - Ser√°n sub-temas espec√≠ficos
        
        - **Corpus diverso** (ej: econom√≠a general)
          - Espera: 50-150 t√≥picos
          - Cubrir√°n m√∫ltiples √°reas
        
        **Reglas emp√≠ricas**:
        - **10-40 t√≥picos**: Macro-temas, bueno para overview
        - **40-80 t√≥picos**: Balance, recomendado
        - **80-150 t√≥picos**: Temas muy espec√≠ficos, an√°lisis detallado
        - **>200 t√≥picos**: Posible sobre-segmentaci√≥n
        
        ---
        
        ##### üîç Analizar Calidad de T√≥picos
        
        **M√©trica 1: Coherencia de Palabras**
        
        ¬øLas palabras del t√≥pico tienen sentido juntas?
        
        **Ejemplo bueno**:
        ```
        ["empleo", "paro", "desempleo", "laboral", "trabajadores"]
        ‚Üí Alta coherencia ‚úÖ
        ```
        
        **Ejemplo malo**:
        ```
        ["banco", "casa", "verde", "datos", "puede"]
        ‚Üí Baja coherencia ‚ùå
        ```
        
        **M√©trica 2: Tama√±o del T√≥pico**
        
        ```
        - <1% del corpus: Posible ruido o muy espec√≠fico
        - 1-5%: T√≥pico espec√≠fico bien definido ‚úÖ
        - 5-15%: T√≥pico importante ‚úÖ‚úÖ
        - >20%: Posible t√≥pico muy general (revisar)
        ```
        
        **M√©trica 3: Documentos Representativos**
        
        Lee los 5-10 documentos m√°s representativos:
        - ¬øTodos tratan del mismo tema?
        - ¬øLa interpretaci√≥n del t√≥pico es clara?
        - ¬øLos documentos confirman las palabras clave?
        
        ---
        
        ##### üìä Interpretar el Gr√°fico 3D (UMAP)
        
        **Ejes X, Y, Z**: NO tienen significado interpretable
        - Son dimensiones reducidas abstractas
        - Solo importan las **distancias relativas**
        
        **¬øQu√© S√ç interpretar?**
        
        - **Clusters bien separados** ‚Üí T√≥picos distintos ‚úÖ
        - **Clusters mezclados** ‚Üí T√≥picos relacionados o ambiguos ‚ö†Ô∏è
        - **Puntos aislados** ‚Üí Outliers o documentos √∫nicos
        - **Densidad del cluster** ‚Üí Coherencia interna del t√≥pico
        
        **Ejemplo**:
        ```
        Si ves:
        - Cluster A (azul) muy separado de cluster B (rojo)
          ‚Üí T√≥picos muy diferentes (ej: "inflaci√≥n" vs "empleo")
        
        - Cluster C (verde) cerca de cluster D (amarillo)
          ‚Üí T√≥picos relacionados (ej: "BCE" vs "pol√≠tica monetaria")
        ```
        
        ---
        
        ##### üìÖ Interpretar An√°lisis Temporal
        
        **Gr√°fico de series de tiempo muestra**:
        - N√∫mero de documentos por t√≥pico en cada per√≠odo
        
        **Patrones a identificar**:
        
        **1. T√≥picos Emergentes** üìà
        ```
        L√≠nea ascendente constante
        ‚Üí Tema cada vez m√°s mencionado
        ‚Üí Ej: "inflaci√≥n" 2021-2023
        ```
        
        **2. T√≥picos Decrecientes** üìâ
        ```
        L√≠nea descendente
        ‚Üí Tema perdiendo relevancia
        ‚Üí Ej: "QE" despu√©s de 2020
        ```
        
        **3. T√≥picos Estacionales** üîÑ
        ```
        Picos peri√≥dicos
        ‚Üí Eventos recurrentes
        ‚Üí Ej: "presupuestos" cada oto√±o
        ```
        
        **4. Eventos Puntuales** üìå
        ```
        Pico abrupto √∫nico
        ‚Üí Crisis o evento espec√≠fico
        ‚Üí Ej: "COVID-19" marzo 2020
        ```
        
        ---
        
        ##### ‚ö†Ô∏è Limitaciones y Consideraciones
        
        **1. T√≥picos no son categor√≠as absolutas**
        - Un documento puede estar "entre" t√≥picos
        - Interpretaci√≥n requiere contexto
        
        **2. Outliers son normales**
        - 5-15% de documentos pueden ser outliers (-1)
        - Son documentos √∫nicos o muy espec√≠ficos
        - NO significa que el modelo fall√≥
        
        **3. Nomenclatura es subjetiva**
        - Debes **T√ö** interpretar y nombrar t√≥picos
        - Las palabras clave son gu√≠a, no etiquetas absolutas
        
        **4. Reproducibilidad parcial**
        - UMAP tiene componente aleatorio
        - Peque√±as variaciones son normales
        - Tendencias principales deben ser consistentes
        
        **5. Contexto importa**
        - Conocimiento del dominio ayuda a interpretar
        - No conf√≠es ciegamente en las palabras
        - Valida con documentos representativos
        
        ---
        
        ##### üéØ Checklist de Validaci√≥n
        
        Para cada t√≥pico, preg√∫ntate:
        
        - [ ] ¬øLas palabras clave tienen sentido juntas?
        - [ ] ¬øPuedo dar un nombre descriptivo al t√≥pico?
        - [ ] ¬øLos documentos representativos confirman mi interpretaci√≥n?
        - [ ] ¬øEl tama√±o del t√≥pico es razonable (>1% corpus)?
        - [ ] ¬øLa evoluci√≥n temporal tiene sentido?
        
        Si 4/5 son ‚úÖ ‚Üí T√≥pico v√°lido
        Si <3/5 son ‚úÖ ‚Üí Revisar o descartar
        """)


def render_quick_start_help():
    """Gu√≠a de inicio r√°pido"""
    st.markdown("### üöÄ Inicio R√°pido")
    
    st.markdown("""
    <div class="info-card">
    <strong>¬øPrimera vez usando la aplicaci√≥n?</strong><br>
    Sigue estos pasos para empezar a analizar t√≥picos en menos de 5 minutos.
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("""
    #### Flujo de Trabajo en 3 Pasos
    
    1. **üéØ Entrenar Modelo** (15-30 minutos)
       - Ve a la pesta√±a "üéØ Entrenar Modelo"
       - Selecciona un preset (recomendado: "An√°lisis General")
       - Click en "üöÄ Entrenar Modelo"
       - Espera a que termine (ver√°s una barra de progreso)
    
    2. **üìä Explorar Resultados** (5-10 minutos)
       - Ve a la pesta√±a "üìä Explorar Resultados"
       - Selecciona el modelo entrenado
       - Explora:
         - Gr√°fico 3D interactivo de t√≥picos
         - WordClouds de palabras clave
         - Series temporales
         - Documentos representativos
    
    3. **üì• Descargar Resultados** (1 minuto)
       - En la secci√≥n "Exportaci√≥n de Resultados"
       - Click en "üì• Descargar Resumen Excel"
       - Abre el archivo en Excel
       - ¬°Listo para presentar!
    
    ---
    
    #### ‚è±Ô∏è Tiempos Estimados
    
    | Tarea | Tiempo |
    |-------|--------|
    | Entrenar modelo (R√°pido) | 10-15 min |
    | Entrenar modelo (Est√°ndar) | 15-25 min |
    | Entrenar modelo (Detallado) | 30-45 min |
    | Explorar resultados | 5-10 min |
    | Exportar a Excel | 1-2 min |
    
    ---
    
    #### ‚úÖ Checklist de Primera Vez
    
    - [ ] Aplicaci√≥n iniciada correctamente
    - [ ] Archivos de datos verificados (ver pesta√±a "üìä Datos y Configuraci√≥n")
    - [ ] Primer modelo entrenado exitosamente
    - [ ] Resultados explorados visualmente
    - [ ] Excel descargado y abierto
    
    """)
    
    st.success("üí° **Consejo**: Empieza con el preset 'An√°lisis General' para tu primer modelo.")


def render_data_info_help():
    """Informaci√≥n sobre los datos y configuraci√≥n"""
    st.markdown("### üìä Datos y Configuraci√≥n del Sistema")
    
    # Verificar archivos de datos
    st.markdown("#### üìÅ Verificaci√≥n de Archivos")
    
    data_dir = Path(__file__).parent.parent / "data"
    noticias_file = data_dir / "noticias.csv"
    embeddings_file = data_dir / "embeddings_precalculados.npz"
    
    col1, col2 = st.columns(2)
    
    with col1:
        if noticias_file.exists():
            file_size = noticias_file.stat().st_size / (1024**2)  # MB
            st.success(f"‚úÖ **noticias.csv** encontrado ({file_size:.0f} MB)")
            
            # Leer informaci√≥n del dataset
            try:
                df_sample = pd.read_csv(noticias_file, nrows=1000)
                df_info = pd.read_csv(noticias_file, usecols=['date'])
                
                st.info(f"""
                **Informaci√≥n del Dataset:**
                - üìÑ Documentos: ~{len(df_info):,}
                - üìÖ Fecha inicial: {df_info['date'].min()}
                - üìÖ Fecha final: {df_info['date'].max()}
                - üìã Columnas: {', '.join(df_sample.columns[:5].tolist())}...
                """)
            except Exception as e:
                st.warning(f"No se pudo leer informaci√≥n detallada: {e}")
        else:
            st.error("‚ùå **noticias.csv** NO encontrado")
            st.markdown("""
            **Soluci√≥n:**
            1. Descarga el archivo desde: [ENLACE]
            2. Col√≥calo en la carpeta `data/`
            3. Reinicia la aplicaci√≥n
            """)
    
    with col2:
        if embeddings_file.exists():
            file_size = embeddings_file.stat().st_size / (1024**2)  # MB
            st.success(f"‚úÖ **embeddings_precalculados.npz** encontrado ({file_size:.0f} MB)")
            
            try:
                embeddings_data = np.load(embeddings_file)
                st.info(f"""
                **Informaci√≥n de Embeddings:**
                - üß† Vectores: {embeddings_data['embeddings'].shape[0]:,}
                - üìè Dimensiones: {embeddings_data['embeddings'].shape[1]}
                - üíæ Tama√±o en memoria: ~{embeddings_data['embeddings'].nbytes / (1024**2):.0f} MB
                """)
            except Exception as e:
                st.warning(f"No se pudo leer informaci√≥n detallada: {e}")
        else:
            st.error("‚ùå **embeddings_precalculados.npz** NO encontrado")
            st.markdown("""
            **Soluci√≥n:**
            1. Descarga el archivo desde: [ENLACE]
            2. Col√≥calo en la carpeta `data/`
            3. Reinicia la aplicaci√≥n
            """)
    
    st.markdown("---")
    
    # Informaci√≥n sobre el formato esperado
    st.markdown("#### üìù Formato de Datos Esperado")
    
    st.markdown("""
    **Archivo: noticias.csv**
    
    El archivo CSV debe tener las siguientes columnas:
    
    | Columna | Tipo | Descripci√≥n | Requerida |
    |---------|------|-------------|-----------|
    | `body` o `text` | string | Texto completo de la noticia | ‚úÖ S√≠ |
    | `date` | datetime | Fecha de publicaci√≥n (YYYY-MM-DD) | ‚úÖ S√≠ |
    | `title` | string | T√≠tulo de la noticia | ‚ö†Ô∏è Opcional |
    | `url` | string | URL de la noticia | ‚ö†Ô∏è Opcional |
    | `doc_id` | int/string | Identificador √∫nico | ‚ö†Ô∏è Opcional |
    
    **Ejemplo de filas:**
    ```csv
    body,date,title
    "El Banco Central Europeo mantiene los tipos...",2023-01-15,"BCE mantiene tipos"
    "La inflaci√≥n en la zona euro alcanza...",2023-01-20,"Inflaci√≥n r√©cord"
    ```
    
    ---
    
    **Archivo: embeddings_precalculados.npz**
    
    Formato NumPy comprimido con:
    - `embeddings`: Matriz de vectores (N √ó 300)
    - N = n√∫mero de documentos
    - 300 = dimensiones del embedding Doc2Vec
    
    ‚ö†Ô∏è **Importante**: El n√∫mero de embeddings debe coincidir con el n√∫mero de documentos en `noticias.csv`
    """)
    
    st.markdown("---")
    
    # Informaci√≥n del sistema
    st.markdown("#### üíª Informaci√≥n del Sistema")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        ram_total = psutil.virtual_memory().total / (1024**3)  # GB
        ram_available = psutil.virtual_memory().available / (1024**3)  # GB
        ram_percent = psutil.virtual_memory().percent
        
        st.metric("RAM Total", f"{ram_total:.1f} GB")
        st.metric("RAM Disponible", f"{ram_available:.1f} GB")
        if ram_percent > 80:
            st.warning(f"‚ö†Ô∏è Uso de RAM: {ram_percent:.0f}%")
        else:
            st.info(f"Uso de RAM: {ram_percent:.0f}%")
    
    with col2:
        cpu_count = psutil.cpu_count()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        st.metric("CPUs", cpu_count)
        st.metric("Uso CPU", f"{cpu_percent:.1f}%")
    
    with col3:
        disk = psutil.disk_usage('.')
        disk_free = disk.free / (1024**3)  # GB
        
        st.metric("Espacio Disponible", f"{disk_free:.1f} GB")
    
    st.markdown("""
    **Requisitos Recomendados:**
    - üíæ RAM: 16 GB (m√≠nimo 8 GB)
    - üñ•Ô∏è CPU: 4 n√∫cleos o m√°s
    - üìÄ Disco: 10 GB libres
    """)


def render_usage_help():
    """Gu√≠a de uso detallada"""
    st.markdown("### üéØ C√≥mo Usar la Aplicaci√≥n")
    
    usage_subtab1, usage_subtab2, usage_subtab3 = st.tabs([
        "Entrenar Modelos",
        "Explorar Resultados",
        "Casos de Uso"
    ])
    
    with usage_subtab1:
        st.markdown("""
        #### üéØ Entrenar un Modelo Top2Vec
        
        **1. Seleccionar Preset**
        
        Los presets est√°n optimizados para diferentes necesidades:
        
        - **An√°lisis General (Recomendado)**
          - ‚è±Ô∏è Tiempo: 15-25 min
          - üìä T√≥picos: 40-80
          - üéØ Uso: Primer an√°lisis, exploraci√≥n inicial
        
        - **Temas Emergentes**
          - ‚è±Ô∏è Tiempo: 10-15 min
          - üìä T√≥picos: 20-40
          - üéØ Uso: Identificar tendencias r√°pidamente
        
        - **Macro-Temas**
          - ‚è±Ô∏è Tiempo: 30-45 min
          - üìä T√≥picos: 80-150
          - üéØ Uso: An√°lisis detallado y profundo
        
        - **Personalizado**
          - ‚è±Ô∏è Tiempo: Variable
          - üìä T√≥picos: Seg√∫n configuraci√≥n
          - üéØ Uso: Usuarios avanzados
        
        ---
        
        **2. Configurar Par√°metros (Opcional)**
        
        Si seleccionas "Personalizado", puedes ajustar:
        
        - **min_count**: Frecuencia m√≠nima de palabras (5-50)
          - M√°s alto = menos ruido, menos t√≥picos
          - M√°s bajo = m√°s t√≥picos, puede incluir ruido
        
        - **umap_n_neighbors**: Vecinos en UMAP (5-50)
          - M√°s alto = estructura global
          - M√°s bajo = estructura local
        
        - **hdbscan_min_cluster_size**: Tama√±o m√≠nimo de cluster (30-200)
          - M√°s alto = menos t√≥picos, m√°s generales
          - M√°s bajo = m√°s t√≥picos, m√°s espec√≠ficos
        
        ---
        
        **3. Iniciar Entrenamiento**
        
        - Click en "üöÄ Entrenar Modelo"
        - Ver√°s una barra de progreso
        - **NO cierres la ventana** del navegador
        - Espera a ver el mensaje de √©xito
        
        ---
        
        **4. Guardar el Modelo**
        
        - El modelo se guarda autom√°ticamente en `modelos/`
        - Nombre con timestamp: `top2vec_model_YYYYMMDD_HHMMSS.model`
        - Tambi√©n se guarda la configuraci√≥n usada en JSON
        """)
    
    with usage_subtab2:
        st.markdown("""
        #### üìä Explorar Resultados
        
        **1. Seleccionar Modelo**
        
        - Usa el dropdown para elegir un modelo entrenado
        - Ver√°s informaci√≥n b√°sica: fecha, t√≥picos, documentos
        
        ---
        
        **2. Gr√°fico 3D Interactivo**
        
        - **Rotar**: Click izquierdo + arrastrar
        - **Zoom**: Scroll del mouse
        - **Pan**: Click derecho + arrastrar
        - **Hover**: Ver informaci√≥n de cada punto
        
        Los colores representan diferentes t√≥picos.
        
        ---
        
        **3. WordClouds de T√≥picos**
        
        - Selecciona un t√≥pico del dropdown
        - Ver√°s una nube de palabras:
          - Tama√±o = importancia de la palabra
          - Color = categor√≠a (decorativo)
        - Debajo ver√°s las palabras exactas con scores
        
        ---
        
        **4. An√°lisis Temporal**
        
        - Gr√°fico de l√≠neas mostrando evoluci√≥n en el tiempo
        - Selecciona m√∫ltiples t√≥picos para comparar
        - Identifica:
          - üìà T√≥picos emergentes (l√≠neas ascendentes)
          - üìâ T√≥picos decrecientes (l√≠neas descendentes)
          - üîÑ T√≥picos estacionales (picos peri√≥dicos)
        
        ---
        
        **5. Documentos Representativos**
        
        - Ver los documentos m√°s representativos de cada t√≥pico
        - √ötil para entender el contexto
        - Incluye fecha y score de similitud
        
        ---
        
        **6. B√∫squeda de Documentos**
        
        - Escribe un tema o query
        - Encuentra los documentos m√°s similares
        - Ejemplo: "pol√≠tica monetaria", "inflaci√≥n", "tipos de inter√©s"
        
        ---
        
        **7. Exportar Resultados**
        
        Tres formatos disponibles:
        
        - **Excel (.xlsx)**: Ideal para presentaciones
          - Hoja 1: Resumen de t√≥picos
          - Hoja 2: Documentos representativos
        
        - **CSV (.csv)**: Para an√°lisis adicional
          - Compatible con R, Python, Excel
        
        - **Embeddings (.npz)**: Para an√°lisis avanzado
          - Vectores 3D de UMAP
        """)
    
    with usage_subtab3:
        st.markdown("""
        #### üíº Casos de Uso Pr√°cticos
        
        **1. An√°lisis de Comunicaciones del BCE**
        
        Objetivo: Identificar temas principales en las √∫ltimas 500 comunicaciones
        
        Flujo:
        1. Entrenar modelo con preset "An√°lisis General"
        2. Identificar top 10 t√≥picos
        3. Analizar evoluci√≥n temporal
        4. Exportar a Excel para briefing ejecutivo
        
        Tiempo total: ~30 minutos
        
        ---
        
        **2. Seguimiento de Inflaci√≥n en Prensa**
        
        Objetivo: Ver c√≥mo ha evolucionado la cobertura de inflaci√≥n
        
        Flujo:
        1. Entrenar modelo con preset "Temas Emergentes"
        2. Usar b√∫squeda sem√°ntica: "inflaci√≥n"
        3. Ver an√°lisis temporal del t√≥pico identificado
        4. Comparar con t√≥picos relacionados ("precios", "IPC")
        
        Tiempo total: ~20 minutos
        
        ---
        
        **3. Preparaci√≥n de Reporte Mensual**
        
        Objetivo: Reporte mensual de temas emergentes
        
        Flujo:
        1. Entrenar modelo cada mes
        2. Comparar con modelo del mes anterior
        3. Identificar t√≥picos nuevos o crecientes
        4. Exportar wordclouds para presentaci√≥n
        
        Tiempo total: ~40 minutos/mes
        
        ---
        
        **4. Investigaci√≥n sobre Pol√≠tica Monetaria**
        
        Objetivo: Analizar discursos sobre pol√≠tica monetaria 2020-2025
        
        Flujo:
        1. Filtrar dataset por fechas
        2. Entrenar con preset "Macro-Temas"
        3. Analizar evoluci√≥n de t√≥picos clave
        4. Exportar documentos representativos para an√°lisis cualitativo
        
        Tiempo total: ~1 hora
        """)


def render_faq_help():
    """Preguntas frecuentes"""
    st.markdown("### ‚ùì Preguntas Frecuentes")
    
    with st.expander("‚ùì ¬øQu√© es Top2Vec y c√≥mo funciona?"):
        st.markdown("""
        **Top2Vec** es un algoritmo de descubrimiento autom√°tico de t√≥picos que:
        
        1. Crea embeddings (representaciones vectoriales) de documentos
        2. Reduce dimensionalidad con UMAP
        3. Agrupa documentos similares con HDBSCAN
        4. Identifica t√≥picos a partir de los clusters
        
        **Ventajas vs LDA tradicional:**
        - ‚úÖ Detecta autom√°ticamente el n√∫mero de t√≥picos
        - ‚úÖ Usa embeddings sem√°nticos (captura significado)
        - ‚úÖ No requiere preprocesamiento intensivo
        - ‚úÖ Mejor con textos cortos y datasets peque√±os
        """)
    
    with st.expander("‚ùì ¬øCu√°nto tiempo tarda entrenar un modelo?"):
        st.markdown("""
        Depende del preset y del tama√±o del dataset:
        
        | Preset | Documentos | Tiempo Estimado |
        |--------|------------|-----------------|
        | Temas Emergentes | 10K-50K | 10-15 min |
        | An√°lisis General | 10K-50K | 15-25 min |
        | Macro-Temas | 10K-50K | 30-45 min |
        
        **Factores que afectan:**
        - CPU: M√°s n√∫cleos = m√°s r√°pido
        - RAM: M√°s RAM = menos swapping
        - Dataset: M√°s documentos = m√°s tiempo
        """)
    
    with st.expander("‚ùì ¬øCu√°ntos t√≥picos debo esperar?"):
        st.markdown("""
        El n√∫mero de t√≥picos se detecta autom√°ticamente, pero t√≠picamente:
        
        - **Temas Emergentes**: 20-40 t√≥picos
        - **An√°lisis General**: 40-80 t√≥picos
        - **Macro-Temas**: 80-150 t√≥picos
        
        Depende de:
        - Diversidad del corpus
        - Par√°metros (min_cluster_size, etc.)
        - Calidad de los embeddings
        
        **¬øMuy pocos t√≥picos?** ‚Üí Reduce `min_cluster_size`  
        **¬øDemasiados t√≥picos?** ‚Üí Aumenta `min_cluster_size`
        """)
    
    with st.expander("‚ùì ¬øPuedo usar mis propios datos?"):
        st.markdown("""
        **S√≠**, solo necesitas:
        
        1. **Archivo CSV** con columnas:
           - `text` o `body`: Texto de los documentos
           - `date`: Fecha (YYYY-MM-DD)
        
        2. **Embeddings precalculados** (opcional):
           - Si no los tienes, puedes generarlos
           - Ver: `utils/save_embeddings.py`
           - Tarda 2-3 horas para 50K documentos
        
        3. **Colocar archivos** en `data/`:
           - `data/tus_documentos.csv`
           - `data/tus_embeddings.npz`
        
        4. **Modificar** `src/configuracion.py`:
           - Cambiar rutas de archivos
        """)
    
    with st.expander("‚ùì ¬øLos resultados son reproducibles?"):
        st.markdown("""
        **Parcialmente**:
        
        - ‚úÖ **Embeddings**: S√≠ (si usas los precalculados)
        - ‚ö†Ô∏è **UMAP**: No completamente (tiene aleatoriedad)
        - ‚ö†Ô∏è **HDBSCAN**: Mayormente s√≠
        
        **Para mayor reproducibilidad:**
        1. Usa `random_state` fijo en configuraci√≥n
        2. Guarda el modelo entrenado
        3. Documenta la versi√≥n de paquetes usados
        
        **Nota**: Peque√±as variaciones son normales y no afectan conclusiones principales.
        """)
    
    with st.expander("‚ùì ¬øPuedo comparar m√∫ltiples modelos?"):
        st.markdown("""
        **S√≠**, puedes:
        
        1. Entrenar varios modelos con diferentes configuraciones
        2. Cada modelo se guarda con timestamp √∫nico
        3. En "Explorar Resultados", selecciona del dropdown
        
        **Comparaci√≥n manual:**
        - N√∫mero de t√≥picos encontrados
        - Coherencia de wordclouds
        - Distribuci√≥n temporal
        
        **Pr√≥ximamente**: Pesta√±a de comparaci√≥n autom√°tica
        """)
    
    with st.expander("‚ùì ¬øQu√© idiomas soporta?"):
        st.markdown("""
        Top2Vec funciona con **cualquier idioma**, pero:
        
        - **Espa√±ol**: ‚úÖ Totalmente soportado (este dataset)
        - **Ingl√©s**: ‚úÖ Totalmente soportado
        - **Otros**: ‚ö†Ô∏è Depende de los embeddings
        
        **Para usar otro idioma:**
        1. Entrenar embeddings en ese idioma
        2. O usar modelos preentrenados multiling√ºes
        3. Ajustar stopwords si es necesario
        """)


def render_troubleshooting_help():
    """Soluci√≥n de problemas"""
    st.markdown("### üîß Soluci√≥n de Problemas")
    
    st.markdown("""
    #### ‚ùå Errores Comunes y Soluciones
    """)
    
    with st.expander("‚ùå 'FileNotFoundError: noticias.csv'"):
        st.markdown("""
        **Causa**: No se encuentra el archivo de datos
        
        **Soluci√≥n**:
        1. Verifica que `data/noticias.csv` existe
        2. Descarga el archivo si falta
        3. Verifica permisos de lectura
        4. Reinicia la aplicaci√≥n
        
        **Verificaci√≥n r√°pida**:
        ```python
        import os
        print(os.path.exists('data/noticias.csv'))  # Debe ser True
        ```
        """)
    
    with st.expander("‚ùå 'MemoryError' durante entrenamiento"):
        st.markdown("""
        **Causa**: RAM insuficiente
        
        **Soluci√≥n inmediata**:
        1. Cierra otros programas
        2. Usa preset "Temas Emergentes" (consume menos RAM)
        3. Reduce el dataset (usa una muestra)
        
        **Soluci√≥n a largo plazo**:
        - Aumenta RAM a 16 GB
        - Usa una m√°quina con m√°s recursos
        - Procesa el dataset en lotes
        """)
    
    with st.expander("‚ùå El modelo tarda demasiado (>1 hora)"):
        st.markdown("""
        **Causas posibles**:
        - CPU lento (pocos n√∫cleos)
        - Par√°metros muy exigentes
        - Dataset muy grande
        
        **Soluciones**:
        1. Usa preset "Temas Emergentes" (m√°s r√°pido)
        2. Reduce `umap_n_neighbors`
        3. Aumenta `min_cluster_size`
        4. Verifica que no hay otros procesos consumiendo CPU
        
        **Normal**: 15-30 min en CPU moderno (i5/i7)
        """)
    
    with st.expander("‚ùå 'No topics found' (0 t√≥picos)"):
        st.markdown("""
        **Causa**: Par√°metros muy restrictivos
        
        **Soluci√≥n**:
        1. Reduce `min_cluster_size` (prueba 30-50)
        2. Reduce `min_count` (prueba 5-10)
        3. Aumenta `umap_n_neighbors`
        4. Verifica que el dataset tiene suficiente diversidad
        
        **Prueba r√°pida**: Usa preset "An√°lisis General"
        """)
    
    with st.expander("‚ùå Demasiados t√≥picos (>200)"):
        st.markdown("""
        **Causa**: Par√°metros muy permisivos
        
        **Soluci√≥n**:
        1. Aumenta `min_cluster_size` (prueba 100-150)
        2. Aumenta `min_count` (prueba 30-50)
        3. Reduce `umap_n_neighbors`
        
        **Nota**: T√≥picos muy espec√≠ficos pueden ser √∫tiles para an√°lisis detallado.
        """)
    
    with st.expander("‚ùå La aplicaci√≥n se congela"):
        st.markdown("""
        **Durante entrenamiento**: Normal, espera pacientemente
        
        **Otros casos**:
        1. Refresca el navegador (F5)
        2. Verifica RAM disponible
        3. Cierra otras pesta√±as del navegador
        4. Reinicia la aplicaci√≥n (Ctrl+C en terminal)
        
        **Si persiste**:
        - Revisa logs en la terminal
        - Verifica errores de Python
        - Contacta soporte t√©cnico
        """)
    
    with st.expander("‚ùå 'Port 8501 already in use'"):
        st.markdown("""
        **Causa**: Ya hay una instancia corriendo
        
        **Soluci√≥n**:
        1. Ve a http://localhost:8501 (puede que ya est√© abierta)
        2. O cierra la terminal anterior
        3. O usa otro puerto:
           ```bash
           streamlit run app.py --server.port 8502
           ```
        """)
    
    with st.expander("‚ùå Los wordclouds no se ven bien"):
        st.markdown("""
        **Causas posibles**:
        - Navegador no soporta im√°genes
        - Error en generaci√≥n de imagen
        
        **Soluci√≥n**:
        1. Actualiza el navegador
        2. Prueba en Chrome o Firefox
        3. Verifica que la librer√≠a `wordcloud` est√© instalada:
           ```bash
           pip install wordcloud
           ```
        """)
    
    st.markdown("---")
    
    st.markdown("""
    #### üìß Contacto de Soporte
    
    Si ninguna soluci√≥n funciona:
    
    1. **Revisa la documentaci√≥n completa**: `MANUAL_USUARIO.md`
    2. **Busca en FAQ t√©cnico**: `src/FAQ.md`
    3. **Abre un issue en GitHub**: [Enlace]
    4. **Contacta al administrador**: [Email]
    
    **Incluye siempre**:
    - Descripci√≥n del problema
    - Mensaje de error completo
    - Pasos para reproducir
    - Informaci√≥n del sistema (ver "üìä Datos y Configuraci√≥n")
    """)


# =============================================================================
# EJECUTAR APLICACI√ìN
# =============================================================================

if __name__ == "__main__":
    main()
